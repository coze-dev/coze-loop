{
  "prompt_ak_ak_get_tip": "ç‚¹å‡»åå‰å¾€ç©ºé—´ç®¡ç†é¡µé¢è·å–",
  "prompt_ak_ak_get_label": "è·å– AK/SK",
  "prompt_usage_call_title": "è°ƒç”¨é…ç½®",
  "prompt_view_detailed_instructions": "æŸ¥çœ‹è¯¦ç»†è¯´æ˜",
  "prompt_choose_language": "é€‰æ‹©è¯­è¨€",
  "prompt_install_sdk": "å®‰è£… SDK",
  "prompt_obtain_prompt_config": "è·å– Prompt é…ç½®",
  "prompt_synchronous_call": "åŒæ­¥è°ƒç”¨",
  "prompt_streaming_call": "æµå¼è°ƒç”¨",
  "prompt_new_version_released_features": "æ–°ç‰ˆæœ¬å·²å‘å¸ƒæˆåŠŸï¼Œå¯ä»¥ç»§ç»­ä½¿ç”¨ä»¥ä¸‹åŠŸèƒ½ï¼š",
  "prompt_performance_evaluation": "æ•ˆæœè¯„ä¼°",
  "prompt_evaluate_prompt_improve_performance": "è¯„ä¼° Prompt æ•ˆæœï¼Œæå‡åº”ç”¨è¡¨ç°",
  "prompt_click_to_view": "ç‚¹å‡»æŸ¥çœ‹",
  "prompt_prompt_invocation": "Prompt è°ƒç”¨",
  "prompt_ptaas_overview_and_limitations": "PTaaSï¼ˆPrompt As a Serviceï¼‰ å³å°†æ‰˜ç®¡çš„ Prompt å‘å¸ƒä¸ºå¯è°ƒç”¨çš„æ¥å£ï¼Œé€šè¿‡é›†æˆ CozeLoop SDK åœ¨ä¸šåŠ¡æµç¨‹ä¸­ç›´æ¥ä¸”å¿«é€Ÿåœ°è°ƒç”¨è¯¥æ¥å£ï¼Œå®ç° Prompt çš„ç‹¬ç«‹è¿­ä»£ä¸è°ƒä¼˜ã€‚PTaaSæš‚æ—¶ä¸æ”¯æŒä¸ºä¸šåŠ¡æ–¹ç‹¬ç«‹éƒ¨ç½²æœåŠ¡ï¼Œç”±CozeLoop æœåŠ¡æä¾›çš„æ¨¡å‹è°ƒç”¨èƒ½åŠ›ã€‚",
  "prompt_prompthub_features_and_integration": "PromptHubï¼šä¸šåŠ¡æ–¹èƒ½å¤Ÿé€šè¿‡é›†æˆ CozeLoop SDK åœ¨ä¸šåŠ¡æœåŠ¡ä¸­æ‹‰å–äº CozeLoop å¹³å°æ‰˜ç®¡çš„ Promptï¼Œè·å– Prompt Template è¯¦ç»†å†…å®¹ï¼Œå¹¶åœ¨ä¸šåŠ¡æœåŠ¡ä¸­è‡ªè¡Œè°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†æˆ–è€…ä¸Einoç­‰Agentæ¡†æ¶è¿›è¡Œé›†æˆã€‚",
  "prompt_integrate_llm_capabilities": "é›†æˆ LLM èƒ½åŠ›",
  "prompt_data_observation": "æ•°æ®è§‚æµ‹",
  "prompt_sdk_data_reporting_and_observation": "æ¥å…¥ SDK ä¸ŠæŠ¥æ•°æ®ï¼Œè¿›è¡Œæ•°æ®è§‚æµ‹",
  "prompt_release_successful": "å‘å¸ƒæˆåŠŸ",
  "prompt_ticket_link": "å·¥å•é“¾æ¥",
  "prompt_service_install_eg": "go get github.com/coze-dev/cozeloop-go",
  "prompt_use_js_installation": "npm i @cozeloop/ai",
  "prompt_use_python_installation": "pip install CozeLoop",
  "prompt_service_config_eg": "package main\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n\n    \"github.com/coze-dev/cozeloop-go\"\n    \"github.com/coze-dev/cozeloop-go/entity\"\n)\n\nfunc main() {\n    // 1.Create a prompt on the platform\n    // You can create a Prompt on the platform's Prompt development page (set Prompt Key to 'prompt_hub_demo'), add the following messages to the template, and submit a version.\n    // System: You are a helpful bot, the conversation topic is {{var1}}.\n    // Placeholder: placeholder1\n    // User: My question is {{var2}}\n    // Placeholder: placeholder2\n\n    ctx := context.Background()\n\n    // Set the following environment variables first.\n    // COZELOOP_WORKSPACE_ID=your workspace id\n    // COZELOOP_API_TOKEN=your token\n    // 2.New loop client\n    client, err := cozeloop.NewClient(\n       // Set whether to report a trace span when get or format prompt.\n       // Default value is false.\n       cozeloop.WithPromptTrace(true))\n    if err != nil {\n       panic(err)\n    }\n\n    llmRunner := llmRunner{\n       client: client,\n    }\n\n    // 1. start root span\n    ctx, span := llmRunner.client.StartSpan(ctx, \"root_span\", \"main_span\", nil)\n\n    // 2. Get the prompt\n    prompt, err := llmRunner.client.GetPrompt(ctx, cozeloop.GetPromptParam{\n       PromptKey: \"prompt_hub_demo\",\n       // If version is not specified, the latest version of the corresponding prompt will be obtained\n       Version: \"0.0.1\",\n    })\n    if err != nil {\n       fmt.Printf(\"get prompt failed: %v\\n\", err)\n       return\n    }\n    if prompt != nil {\n       // Get messages of the prompt\n       if prompt.PromptTemplate != nil {\n          messages, err := json.Marshal(prompt.PromptTemplate.Messages)\n          if err != nil {\n             fmt.Printf(\"json marshal failed: %v\\n\", err)\n             return\n          }\n          fmt.Printf(\"prompt messages=%s\\n\", string(messages))\n       }\n       // Get llm config of the prompt\n       if prompt.LLMConfig != nil {\n          llmConfig, err := json.Marshal(prompt.LLMConfig)\n          if err != nil {\n             fmt.Printf(\"json marshal failed: %v\\n\", err)\n          }\n          fmt.Printf(\"prompt llm config=%s\\n\", llmConfig)\n       }\n\n       // 3. Format messages of the prompt\n       userMessageContent := \"Hello!\"\n       assistantMessageContent := \"Hello!\"\n       messages, err := llmRunner.client.PromptFormat(ctx, prompt, map[string]any{\n          // Normal variable type should be string\n          \"var1\": \"artificial intelligence\",\n          // Placeholder variable type should be entity.Message/*entity.Message/[]entity.Message/[]*entity.Message\n          \"placeholder1\": []*entity.Message{\n             {\n                Role:    entity.RoleUser,\n                Content: &userMessageContent,\n             },\n             {\n                Role:    entity.RoleAssistant,\n                Content: &assistantMessageContent,\n             },\n          },\n          // Other variables in the prompt template that are not provided with corresponding values will be considered as empty values\n       })\n       if err != nil {\n          fmt.Printf(\"prompt format failed: %v\\n\", err)\n          return\n       }\n       data, err := json.Marshal(messages)\n       if err != nil {\n          fmt.Printf(\"json marshal failed: %v\\n\", err)\n          return\n       }\n       fmt.Printf(\"formatted messages=%s\\n\", string(data))\n\n       if err != nil {\n          return\n       }\n    }\n\n    // 4. span finish\n    span.Finish(ctx)\n\n    // 5. (optional) flush or close\n    // -- force flush, report all traces in the queue\n    // Warning! In general, this method is not needed to be call, as spans will be automatically reported in batches.\n    // Note that flush will block and wait for the report to complete, and it may cause frequent reporting,\n    // affecting performance.\n    llmRunner.client.Flush(ctx)\n}\n\ntype llmRunner struct {\n    client cozeloop.Client\n}",
  "prompt_use_js_configuration": "import { PromptHub } from '@cozeloop/ai'; \n \nconst hub = new PromptHub({ \n  /** workspace id, use process.env.COZELOOP_WORKSPACE_ID when unprovided */ \n  // workspaceId: 'your_workspace_id', \n  apiClient: { \n    // baseURL: 'api_base_url', \n    // token: 'your_api_token', \n  }, \n}); \n \n// get prompt with `beta` label \n// - prompt_key: xxx \n// - version: undefined \n// - label: beta \nconst prompt = await hub.getPrompt('xxx', undefined, 'beta'); \n \n// format prompt with variables \nconst messages = hub.formatPrompt(prompt, { \n  var1: 'value_of_var1', \n  var2: 'value_of_var2', \n  var3: 'value_of_var3', \n  placeholder1: { role: 'assistant', content: 'user' }, \n});",
  "prompt_use_python_configuration": "if __name__ == '__main__':\n    # 1.Create a prompt on the platform\n    # You can create a Prompt on the platform's Prompt development page (set Prompt Key to 'prompt_hub_demo'),\n    # add the following messages to the template, and submit a version.\n    # System: You are a helpful bot, the conversation topic is {{var1}}.\n    # Placeholder: placeholder1\n    # User: My question is {{var2}}\n    # Placeholder: placeholder2\n\n    # Set the following environment variables first.\n    # COZELOOP_WORKSPACE_ID=your workspace id\n    # COZELOOP_API_TOKEN=your token\n    # 2.New loop client\n    client = cozeloop.new_client(\n        # Set whether to report a trace span when get or format prompt.\n        # Default value is false.\n        prompt_trace=True)\n\n    # 3. new root span\n    rootSpan = client.start_span(\"root_span\", \"main_span\")\n\n    # 4. Get the prompt\n    # If no specific version is specified, the latest version of the corresponding prompt will be obtained\n    prompt = client.get_prompt(prompt_key=\"prompt_hub_demo\", version=\"0.0.1\")\n    if prompt is not None:\n        # Get messages of the prompt\n        if prompt.prompt_template is not None:\n            messages = prompt.prompt_template.messages\n            print(\n                f\"prompt messages: {json.dumps([message.model_dump(exclude_none=True) for message in messages], ensure_ascii=False)}\")\n        # Get llm config of the prompt\n        if prompt.llm_config is not None:\n            llm_config = prompt.llm_config\n            print(f\"prompt llm_config: {llm_config.model_dump_json(exclude_none=True)}\")\n\n        # 5.Format messages of the prompt\n        formatted_messages = client.prompt_format(prompt, {\n            # Normal variable type should be string\n            \"var1\": \"artificial intelligence\",\n            # Placeholder variable type should be Message/List[Message]\n            \"placeholder1\": [Message(role=Role.USER, content=\"Hello!\"),\n                             Message(role=Role.ASSISTANT, content=\"Hello!\")]\n            # Other variables in the prompt template that are not provided with corresponding values will be\n            # considered as empty values.\n        })\n        print(\n            f\"formatted_messages: {json.dumps([message.model_dump(exclude_none=True) for message in formatted_messages], ensure_ascii=False)}\")\n\n    rootSpan.finish()\n    # 6. (optional) flush or close\n    # -- force flush, report all traces in the queue\n    # Warning! In general, this method is not needed to be call, as spans will be automatically reported in batches.\n    # Note that flush will block and wait for the report to complete, and it may cause frequent reporting,\n    # affecting performance.\n    client.flush()",
  "prompt_call_stream": "package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n\n    \"github.com/coze-dev/cozeloop-go\"\n    \"github.com/coze-dev/cozeloop-go/entity\"\n    \"github.com/coze-dev/cozeloop-go/internal/util\"\n)\n\nfunc main() {\n    // 1.Create a prompt on the platform\n    // Create a Prompt on the platform's Prompt development page (set Prompt Key to 'ptaas_demo'),\n    // add the following messages to the template, submit a version.\n    // System: You are a helpful assistant for {{topic}}.\n    // User: Please help me with {{user_request}}\n    ctx := context.Background()\n\n    // Set the following environment variables first.\n    // COZELOOP_WORKSPACE_ID=your workspace id\n    // COZELOOP_API_TOKEN=your token\n    // 2.New loop client\n    client, err := cozeloop.NewClient()\n    if err != nil {\n       panic(err)\n    }\n    defer client.Close(ctx)\n\n    ctx, span := client.StartSpan(ctx, \"root_span\", \"custom\")\n    defer span.Finish(ctx)\n\n    // 3. Execute prompt\n    executeRequest := &entity.ExecuteParam{\n       PromptKey: \"CozeLoop_Oncall_Master\",\n       Version:   \"0.0.1\",\n       VariableVals: map[string]any{\n          \"topic\":        \"artificial intelligence\",\n          \"user_request\": \"explain what is machine learning\",\n       },\n       // You can also append messages to the prompt.\n       Messages: []*entity.Message{\n          {\n             Role:    entity.RoleUser,\n             Content: util.Ptr(\"Keep the answer brief.\"),\n          },\n       },\n    }\n    // 3.2 stream\n    stream(ctx, client, executeRequest)\n    client.Flush(ctx)\n}\n\nfunc stream(ctx context.Context, client cozeloop.Client, executeRequest *entity.ExecuteParam) {\n    streamReader, err := client.ExecuteStreaming(ctx, executeRequest)\n    if err != nil {\n       panic(err)\n    }\n    for {\n       result, err := streamReader.Recv()\n       if err != nil {\n          if err == io.EOF {\n             fmt.Println(\"\\nStream finished.\")\n             break\n          }\n          panic(err)\n       }\n       printExecuteResult(result)\n    }\n}\n\nfunc printExecuteResult(result entity.ExecuteResult) {\n    if result.Message != nil {\n       fmt.Printf(\"Message: %s\\n\", util.ToJSON(result.Message))\n    }\n    if util.PtrValue(result.FinishReason) != \"\" {\n       fmt.Printf(\"FinishReason: %s\\n\", util.PtrValue(result.FinishReason))\n    }\n    if result.Usage != nil {\n       fmt.Printf(\"Usage: %s\\n\", util.ToJSON(result.Usage))\n    }\n}",
  "prompt_use_js_streaming_call": "import { ApiClient, PromptAsAService } from '@cozeloop/ai'; \n \nconst apiClient = new ApiClient({ \n  token: 'pat_xxx', \n}); \n \nconst model = new PromptAsAService({ \n  // or set it as process.env.COZELOOP_WORKSPACE_ID, \n  workspaceId: 'your_workspace_id', \n  // prompt to invoke as a service \n  prompt: { \n    prompt_key: 'ptaas_demo', \n    version: '0.0.1', \n  }, \n  apiClient, \n}); \n \nconst replyStream = await model.stream({ \n  messages: [{ role: 'user', content: 'Keep the answer brief.' }], \n  variables: { \n    topic: 'artificial intelligence', \n    user_request: 'explain what is machine learning', \n  }, \n}); \n \nfor await (const chunk of replyStream) { \n console.info(chunk); \n}",
  "prompt_use_python_streaming_call": "import asyncio\nimport os\n\nfrom anyio import sleep\n\nfrom cozeloop import new_client, Client\nfrom cozeloop.entities.prompt import Message, Role, ExecuteResult\n\n\ndef setup_client() -> Client:\n    \"\"\"\n    Unified client setup function\n    \n    Environment variables:\n    - COZELOOP_WORKSPACE_ID: workspace ID\n    - COZELOOP_API_TOKEN: API token\n    \"\"\"\n    # Set the following environment variables first.\n    # COZELOOP_WORKSPACE_ID=your workspace id\n    # COZELOOP_API_TOKEN=your token\n    client = new_client(\n        api_base_url=os.getenv(\"COZELOOP_API_BASE_URL\"),\n        workspace_id=os.getenv(\"COZELOOP_WORKSPACE_ID\"),\n        api_token=os.getenv(\"COZELOOP_API_TOKEN\"),\n    )\n    return client\n\n\ndef print_execute_result(result: ExecuteResult) -> None:\n    \"\"\"Unified result printing function, consistent with Go version format\"\"\"\n    if result.message:\n        print(f\"Message: {result.message}\")\n    if result.finish_reason:\n        print(f\"FinishReason: {result.finish_reason}\")\n    if result.usage:\n        print(f\"Usage: {result.usage}\")\n\n\ndef sync_stream_example(client: Client) -> None:\n    \"\"\"Sync stream call example\"\"\"\n    print(\"=== Sync Stream Example ===\")\n    \n    stream_reader = client.execute_prompt(\n        prompt_key=\"ptaas_demo\",\n        version=\"0.0.1\",\n        variable_vals={\n            \"topic\": \"artificial intelligence\",\n            \"user_request\": \"explain what is machine learning\"\n        },\n        messages=[\n            Message(role=Role.USER, content=\"Keep the answer brief.\")\n        ],\n        stream=True\n    )\n    \n    for result in stream_reader:\n        print_execute_result(result)\n    \n    print(\"\\nStream finished.\")\n\n\nasync def async_stream_example(client: Client) -> None:\n    \"\"\"Async stream call example\"\"\"\n    print(\"=== Async Stream Example ===\")\n    \n    stream_reader = await client.aexecute_prompt(\n        prompt_key=\"ptaas_demo\",\n        version=\"0.0.1\",\n        variable_vals={\n            \"topic\": \"artificial intelligence\",\n            \"user_request\": \"explain what is machine learning\"\n        },\n        messages=[\n            Message(role=Role.USER, content=\"Keep the answer brief.\")\n        ],\n        stream=True\n    )\n    \n    async for result in stream_reader:\n        print_execute_result(result)\n    \n    print(\"\\nStream finished.\")\n\n\nasync def main():\n    \"\"\"Main function\"\"\"\n    client = setup_client()\n\n    root_span = client.start_span(\"root\", \"custom\")\n    try:\n        # Sync stream call\n        sync_stream_example(client)\n\n        # Async stream call\n        await async_stream_example(client)\n        \n    finally:\n        # Close client\n        root_span.finish()\n        if hasattr(client, 'close'):\n            client.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
  "prompt_service_call_sync": "package main\n\nimport (\n    \"context\"\n    \"fmt\"\n\n    \"github.com/coze-dev/cozeloop-go\"\n    \"github.com/coze-dev/cozeloop-go/entity\"\n    \"github.com/coze-dev/cozeloop-go/internal/util\"\n)\n\nfunc main() {\n    // 1.Create a prompt on the platform\n    // Create a Prompt on the platform's Prompt development page (set Prompt Key to 'ptaas_demo'),\n    // add the following messages to the template, submit a version.\n    // System: You are a helpful assistant for {{topic}}.\n    // User: Please help me with {{user_request}}\n    ctx := context.Background()\n\n    // Set the following environment variables first.\n    // COZELOOP_WORKSPACE_ID=your workspace id\n    // COZELOOP_API_TOKEN=your token\n    // 2.New loop client\n    client, err := cozeloop.NewClient()\n    if err != nil {\n       panic(err)\n    }\n    defer client.Close(ctx)\n\n    ctx, span := client.StartSpan(ctx, \"root_span\", \"custom\")\n    defer span.Finish(ctx)\n\n    // 3. Execute prompt\n    executeRequest := &entity.ExecuteParam{\n       PromptKey: \"CozeLoop_Oncall_Master\",\n       Version:   \"0.0.1\",\n       VariableVals: map[string]any{\n          \"topic\":        \"artificial intelligence\",\n          \"user_request\": \"explain what is machine learning\",\n       },\n       // You can also append messages to the prompt.\n       Messages: []*entity.Message{\n          {\n             Role:    entity.RoleUser,\n             Content: util.Ptr(\"Keep the answer brief.\"),\n          },\n       },\n    }\n    // 3.1 non stream\n    nonStream(ctx, client, executeRequest)\n    client.Flush(ctx)\n}\n\nfunc nonStream(ctx context.Context, client cozeloop.Client, executeRequest *entity.ExecuteParam) {\n    result, err := client.Execute(ctx, executeRequest)\n    if err != nil {\n       panic(err)\n    }\n    printExecuteResult(result)\n}\n\nfunc printExecuteResult(result entity.ExecuteResult) {\n    if result.Message != nil {\n       fmt.Printf(\"Message: %s\\n\", util.ToJSON(result.Message))\n    }\n    if util.PtrValue(result.FinishReason) != \"\" {\n       fmt.Printf(\"FinishReason: %s\\n\", util.PtrValue(result.FinishReason))\n    }\n    if result.Usage != nil {\n       fmt.Printf(\"Usage: %s\\n\", util.ToJSON(result.Usage))\n    }\n}",
  "prompt_use_js_synchronous_call": "import { ApiClient, PromptAsAService } from '@cozeloop/ai'; \n \nconst apiClient = new ApiClient({ \n  token: 'pat_xxx', \n}); \n \nconst model = new PromptAsAService({ \n  // or set it as process.env.COZELOOP_WORKSPACE_ID, \n  workspaceId: 'your_workspace_id', \n  // prompt to invoke as a service \n  prompt: { \n    prompt_key: 'ptaas_demo', \n    version: '0.0.1', \n  }, \n  apiClient, \n}); \n \nconst reply = await model.invoke({ \n  messages: [{ role: 'user', content: 'Keep the answer brief.' }], \n  variables: { \n    topic: 'artificial intelligence', \n    user_request: 'explain what is machine learning', \n  }, \n}); \n \nconsole.info(reply);",
  "prompt_use_python_simultaneous_call": "import asyncio\nimport os\n\nfrom anyio import sleep\n\nfrom cozeloop import new_client, Client\nfrom cozeloop.entities.prompt import Message, Role, ExecuteResult\n\n\ndef setup_client() -> Client:\n    \"\"\"\n    Unified client setup function\n    \n    Environment variables:\n    - COZELOOP_WORKSPACE_ID: workspace ID\n    - COZELOOP_API_TOKEN: API token\n    \"\"\"\n    # Set the following environment variables first.\n    # COZELOOP_WORKSPACE_ID=your workspace id\n    # COZELOOP_API_TOKEN=your token\n    client = new_client(\n        api_base_url=os.getenv(\"COZELOOP_API_BASE_URL\"),\n        workspace_id=os.getenv(\"COZELOOP_WORKSPACE_ID\"),\n        api_token=os.getenv(\"COZELOOP_API_TOKEN\"),\n    )\n    return client\n\n\ndef print_execute_result(result: ExecuteResult) -> None:\n    \"\"\"Unified result printing function, consistent with Go version format\"\"\"\n    if result.message:\n        print(f\"Message: {result.message}\")\n    if result.finish_reason:\n        print(f\"FinishReason: {result.finish_reason}\")\n    if result.usage:\n        print(f\"Usage: {result.usage}\")\n\n\ndef sync_non_stream_example(client: Client) -> None:\n    \"\"\"Sync non-stream call example\"\"\"\n    print(\"=== Sync Non-Stream Example ===\")\n    \n    # 1. Create a prompt on the platform\n    # Create a Prompt on the platform's Prompt development page (set Prompt Key to 'ptaas_demo'),\n    # add the following messages to the template, submit a version.\n    # System: You are a helpful assistant for {{topic}}.\n    # User: Please help me with {{user_request}}\n    \n    result = client.execute_prompt(\n        prompt_key=\"ptaas_demo\",\n        version=\"0.0.1\",\n        variable_vals={\n            \"topic\": \"artificial intelligence\",\n            \"user_request\": \"explain what is machine learning\"\n        },\n        # You can also append messages to the prompt.\n        messages=[\n            Message(role=Role.USER, content=\"Keep the answer brief.\")\n        ],\n        stream=False\n    )\n    print_execute_result(result)\n\n\nasync def async_non_stream_example(client: Client) -> None:\n    \"\"\"Async non-stream call example\"\"\"\n    print(\"=== Async Non-Stream Example ===\")\n    \n    result = await client.aexecute_prompt(\n        prompt_key=\"ptaas_demo\",\n        version=\"0.0.1\",\n        variable_vals={\n            \"topic\": \"artificial intelligence\",\n            \"user_request\": \"explain what is machine learning\"\n        },\n        messages=[\n            Message(role=Role.USER, content=\"Keep the answer brief.\")\n        ],\n        stream=False\n    )\n    print_execute_result(result)\n\n\nasync def main():\n    \"\"\"Main function\"\"\"\n    client = setup_client()\n\n    root_span = client.start_span(\"root\", \"custom\")\n    try:\n        # Sync non-stream call\n        sync_non_stream_example(client)\n\n        # Async non-stream call\n        await async_non_stream_example(client)\n\n    finally:\n        # Close client\n        root_span.finish()\n        if hasattr(client, 'close'):\n            client.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
  "fornax_prompt_manual_config_video_sample_params": "æ‰‹åŠ¨é…ç½®è§†é¢‘å˜é‡é‡‡æ ·å‚æ•°ï¼Œè¯¦è§",
  "fornax_prompt_documentation": "è¯´æ˜æ–‡æ¡£",
  "fornax_prompt_frame_extraction_config": "æŠ½å¸§é…ç½®",
  "fornax_prompt_fps_influence_on_video_understanding_and_token_usage": "FPS è¶Šé«˜ï¼Œè§†é¢‘ç†è§£è¶Šç²¾ç»†ï¼ŒTokenç”¨é‡è¶Šå¤§",
  "fornax_prompt_please_enter_frame_extraction_config": "è¯·è¾“å…¥æŠ½å¸§é…ç½®",
  "fornax_prompt_video_sampling_config": "è§†é¢‘é‡‡æ ·é…ç½®",
  "fornax_prompt_video_manual_uniform_sampling_support": "è§†é¢‘æ”¯æŒæ‰‹åŠ¨é…ç½®ä¸ºæŒ‰æ—¶é—´å‡åŒ€é‡‡æ ·ï¼Œå³å›ºå®šæ—¶é—´é—´éš”æŠ½å¸§è®¾ç½®æ¯ç§’å¤„ç†çš„å¸§æ•°ï¼ˆFPSï¼‰ã€‚è¯¦è§",
  "prompt_copy_prompt": "å¤åˆ¶Prompt",
  "prompt_name_query": "åç§°æŸ¥è¯¢",
  "prompt_playground_mock_system": "# è§’è‰²\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸è§„åˆ’åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚å’Œåå¥½ï¼Œè¿…é€Ÿä¸”ç²¾å‡†åœ°ä¸ºç”¨æˆ·ç”Ÿæˆå…¨é¢ã€è¯¦ç»†ä¸”ä¸ªæ€§åŒ–çš„æ—…æ¸¸è§„åˆ’æ–‡æ¡£ã€‚\n\n## æŠ€èƒ½ï¼šåˆ¶å®šæ—…æ¸¸è§„åˆ’æ–¹æ¡ˆ\nä¸ºç”¨æˆ·é‡èº«åˆ¶å®šåˆç†ä¸”èˆ’é€‚çš„è¡Œç¨‹å®‰æ’å’Œè´´å¿ƒçš„æ—…è¡ŒæŒ‡å¼•ã€‚å¯¹äºä¸åŒä¸»é¢˜ï¼Œéœ€è¦èƒ½å¤Ÿä½“ç°å¯¹åº”ä¸»é¢˜çš„ç‰¹è‰²ã€éœ€æ±‚æˆ–æ³¨æ„äº‹é¡¹ç­‰ã€‚å¦‚äº²å­æ¸¸ï¼Œéœ€è¦ä½“ç°å¸¦å°å­©æ—…è¡Œé€”ä¸­è¦æ³¨æ„çš„å†…å®¹ï¼Œç”¨æˆ·çš„é¢„ç®—å’Œåå¥½ç­‰ã€‚ \nå›å¤ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼ˆå†…å®¹å¯ä»¥åˆç†ä½¿ç”¨ emoji è¡¨æƒ…ï¼Œè®©å†…å®¹æ›´ç”ŸåŠ¨ï¼‰ï¼š\n\n## è¾“å‡ºæ ¼å¼\n#### åŸºæœ¬ä¿¡æ¯\n- ğŸ›« å‡ºå‘åœ°ï¼š{{departure}}  <å¦‚æœªæä¾›ï¼Œåˆ™ä¸å±•ç¤ºæ­¤ä¿¡æ¯>\n- ğŸ¯ ç›®çš„åœ°ï¼š{{destination}}\n- ğŸ«‚ äººæ•°ï¼š{{people_num}}äºº\n- ğŸ“… å¤©æ•°ï¼š{{days_num}}å¤©\n- ğŸ¨ ä¸»é¢˜ï¼š{{travel_theme}} \n##### <ç›®çš„åœ°>ç®€ä»‹\n<ä»‹ç›®çš„åœ°çš„åŸºæœ¬ä¿¡æ¯ï¼Œçº¦100å­—>\n<æè¿°å¤©æ°”çŠ¶å†µã€ç©¿è¡£æŒ‡å—ï¼Œçº¦100å­—>\n<æè¿°å½“åœ°ç‰¹è‰²é¥®é£Ÿã€é£ä¿—ä¹ æƒ¯ç­‰ï¼Œçº¦100å­—>\n#### Checklist\n- æ‰‹æœºã€å……ç”µå™¨\n<éœ€è¦æºå¸¦çš„ç‰©å“æˆ–å‡†å¤‡äº‹é¡¹ï¼ŒæŒ‰éœ€æ±‚ç”Ÿæˆ>\n#### è¡Œç¨‹å®‰æ’\n<æ ¹æ®ç”¨æˆ·æœŸæœ›å¤©æ•°ï¼ˆ{{days_num}}å¤©ï¼‰å®‰æ’æ¯æ—¥è¡Œç¨‹>\n##### ç¬¬ä¸€å¤©ã€åœ°ç‚¹1 - åœ°ç‚¹2 - ...\n###### è¡Œç¨‹1ï¼šåœ°ç‚¹1\n<åœ°ç‚¹çš„æ™¯ç‚¹ç®€ä»‹ï¼Œçº¦100å­—>\n<åœ°ç‚¹çš„äº¤é€šæ–¹å¼ï¼Œæä¾›åˆç†çš„äº¤é€šæ–¹å¼åŠä½¿ç”¨æ—¶é—´ä¿¡æ¯>\n<åœ°ç‚¹çš„æ¸¸ç©æ–¹å¼ï¼Œæä¾›æ¨èæ¸¸ç©æ—¶é•¿ã€æ¸¸ç©æ–¹å¼ã€æ³¨æ„äº‹é¡¹ã€é¢„å®šä¿¡æ¯ç­‰ï¼Œçº¦100å­—>\n<å¦‚æœ {{days_num}}è¶…è¿‡1å¤©ï¼Œåˆ™ç»§ç»­æŒ‰ç…§ç¬¬ä¸€å¤©æ ¼å¼ç”Ÿæˆ>\n#### æ³¨æ„äº‹é¡¹\n<æ ¹æ®ä»¥ä¸Šæ—¥ç¨‹å®‰æ’ä¿¡æ¯ï¼Œæä¾›ä¸€äº›ç›®çš„åœ°æ—…è¡Œçš„æ³¨æ„äº‹é¡¹>\n\n\n## é™åˆ¶:\n- æ‰€è¾“å‡ºçš„å†…å®¹å¿…é¡»æŒ‰ç…§ç»™å®šçš„æ ¼å¼è¿›è¡Œç»„ç»‡ï¼Œä¸èƒ½åç¦»æ¡†æ¶è¦æ±‚ã€‚",
  "prompt_playground_mock_user": "## ç”¨æˆ·éœ€æ±‚\n- å‡ºå‘åœ°ï¼š{{departure}}  \n- ç›®çš„åœ°ï¼š{{destination}}\n- äººæ•°ï¼š{{people_num}}\n- å¤©æ•°ï¼š{{days_num}}\n- ä¸»é¢˜ï¼š{{travel_theme}} ",
  "prompt_normal_template_var_intro": "Normal æ¨¡ç‰ˆåœ¨Prompt ä¸­è¾“å…¥ {{}}å³å¯åˆ›å»ºå˜é‡ï¼Œå…¶ä»–æ¨¡ç‰ˆè¯·æ‰‹åŠ¨åˆ›å»ºå˜é‡",
  "prompt_additional_configuration": "é¢å¤–é…ç½®",
  "prompt_deep_thinking_switch": "æ·±åº¦æ€è€ƒå¼€å…³",
  "prompt_deep_thinking_length": "æ·±åº¦æ€è€ƒé•¿åº¦",
  "prompt_deep_thinking_degree": "æ·±åº¦æ€è€ƒç¨‹åº¦",
  "prompt_deep_thinking": "æ·±åº¦æ€è€ƒ",
  "prompt_deep_thinking_description": "å¼€å¯åï¼Œæ¨¡å‹ä¼šæ ¹æ®è¾“å…¥çš„é—®é¢˜ï¼Œè¿›è¡Œæ·±åº¦æ€è€ƒï¼Œç”Ÿæˆæ›´å‡†ç¡®çš„å›ç­”ã€‚",
  "prompt_edit_prompt_snippet": "ç¼–è¾‘ Prompt ç‰‡æ®µ",
  "prompt_copy_prompt_snippet": "å¤åˆ¶ Prompt ç‰‡æ®µ",
  "prompt_create_prompt_snippet": "åˆ›å»º Prompt ç‰‡æ®µ",
  "prompt_prompt_snippet_name": "Prompt ç‰‡æ®µåç§°",
  "prompt_prompt_snippet_description": "Prompt ç‰‡æ®µæè¿°",
  "prompt_prompt_key_length_limit": "Prompt Key é•¿åº¦ä¸èƒ½è¶…è¿‡ 100 ä¸ªå­—ç¬¦",
  "prompt_prompt_name_length_limit": "{promptNameLabel}é•¿åº¦ä¸èƒ½è¶…è¿‡ 100 ä¸ªå­—ç¬¦",
  "prompt_modal_title_confirm": "{modalTitle}-ç¡®è®¤",
  "prompt_confirm_delete": "ç¡®å®šåˆ é™¤",
  "prompt_no_change_info": "æš‚æ— å˜æ›´ä¿¡æ¯",
  "prompt_model_id": "æ¨¡å‹ID",
  "prompt_template_type": "æ¨¡æ¿ç±»å‹",
  "prompt_source_version": "æºç‰ˆæœ¬",
  "prompt_prompt_configuration": "Prompt é…ç½®",
  "prompt_prompt_change_count": "Prompt å˜æ›´ ({promptTypeDiffCount})",
  "prompt_config_change_count": "é…ç½®å˜æ›´ ({configTypeDiffCount})",
  "prompt_exit_fullscreen": "é€€å‡ºå…¨å±",
  "prompt_expand_nested_content": "å±•å¼€åµŒå¥—å†…å®¹",
  "prompt_prompt_diff_change_info": "Prompt Diffå˜æ›´ä¿¡æ¯",
  "prompt_previous_diff": "ä¸Šä¸€ä¸ª Diff",
  "prompt_next_diff": "ä¸‹ä¸€ä¸ª Diff",
  "prompt_comparable_versions": "å¯å¯¹æ¯”ç‰ˆæœ¬",
  "prompt_current_version": "ç°è¡Œç‰ˆæœ¬",
  "prompt_enter_fullscreen": "è¿›å…¥å…¨å±",
  "prompt_delete_prompt": "åˆ é™¤Prompt",
  "prompt_latest_committer": "æœ€è¿‘æäº¤äºº",
  "prompt_prompt_snippet": "Prompt ç‰‡æ®µ",
  "prompt_blank_prompt": "ç©ºç™½ Prompt",
  "prompt_prompt_snippet_nesting_support": "Promptç‰‡æ®µæ”¯æŒè¢«åµŒå¥—åœ¨ä¸åŒçš„Prompt Templateé‡Œå¤ç”¨",
  "prompt_view_documentation": "æŸ¥çœ‹æ–‡æ¡£",
  "prompt_current_submission_draft": "æœ¬æ¬¡æäº¤è‰ç¨¿ç‰ˆæœ¬",
  "prompt_submit_no_version_diff_confirm": "æœ¬æ¬¡æäº¤æ— ç‰ˆæœ¬å·®å¼‚ï¼Œç¡®å®šè¦æäº¤ä¹ˆ",
  "prompt_prompt_submit": "Prompt æäº¤",
  "prompt_no_prompt_snippet": "æš‚æ—  Prompt ç‰‡æ®µ",
  "prompt_prompt_snippet_reuse_support": "Prompt ç‰‡æ®µæ”¯æŒåœ¨ä¸åŒçš„ Prompt Template é‡Œå¤ç”¨",
  "prompt_full_process_prompt_support": "æä¾›ä»ç¼–å†™ã€è°ƒè¯•ã€ä¼˜åŒ–åˆ°ç‰ˆæœ¬ç®¡ç†çš„æç¤ºè¯å…¨æµç¨‹æ”¯æŒï¼Œç‚¹å‡»å³å¯åˆ›å»º",
  "prompt_snippet_version": "ç‰‡æ®µç‰ˆæœ¬",
  "prompt_reference_snippet_prompt_version": "å¼•ç”¨æ­¤ç‰‡æ®µPrompt ç‰ˆæœ¬",
  "prompt_snippet_reference_records": "ç‰‡æ®µè¢«å¼•ç”¨è®°å½•",
  "prompt_total_reference_projects": "å…± {totalReferenceCount} ä¸ªé¡¹ç›®",
  "prompt_cannot_delete_snippet_variables": "åªèƒ½åˆ é™¤ Prompt é‡Œé¢çš„å˜é‡ï¼Œç‰‡æ®µä¸­çš„å˜é‡æ— æ³•è¢«åˆ é™¤",
  "prompt_snippet_variables_no_delete": "ç‰‡æ®µé‡Œçš„å˜é‡ä¸æ”¯æŒåˆ é™¤",
  "prompt_number_of_snippets": "{placeholder1}ä¸ªç‰‡æ®µ",
  "prompt_variable_referenced_in_snippets": "æ­¤å˜é‡åœ¨{snippetNames}ç‰‡æ®µä¸­è¢«å¼•ç”¨",
  "prompt_reference_project": "å¼•ç”¨é¡¹ç›®",
  "prompt_multi_turn_conversation": "å¤šè½®å¯¹è¯",
  "prompt_single_turn_conversation": "å•è½®å¯¹è¯",
  "prompt_new_message": "æ–°å¢æ¶ˆæ¯",
  "prompt_common_configuration": "å¸¸ç”¨é…ç½®",
  "prompt_compare_versions": "å¯¹æ¯”ç‰ˆæœ¬",
  "prompt_currently_editing_version": "æ­£åœ¨ç¼–è¾‘çš„ç‰ˆæœ¬",
  "prompt_exit_diff": "é€€å‡º Diff",
  "prompt_enter_diff": "è¿›å…¥ Diff",
  "prompt_no_submitted_versions_no_compare": "æš‚æ— å·²æäº¤ç‰ˆæœ¬ï¼Œä¸æ”¯æŒè¿›è¡Œå¯¹æ¯”",
  "prompt_gotemplate_engine": "GoTemplate æ¨¡æ¿å¼•æ“",
  "prompt_prompt_contains_mismatched_snippet": "Prompt ä¸­åŒ…å«ä¸åŒ¹é…æ¨¡æ¿ç±»å‹çš„ç‰‡æ®µ",
  "prompt_compare_mode": "å¯¹æ¯”æ¨¡å¼",
  "prompt_enter_compare_mode": "è¿›å…¥å¯¹æ¯”æ¨¡å¼",
  "prompt_open_version_history": "æ‰“å¼€ç‰ˆæœ¬è®°å½•",
  "prompt_submit_new_version": "æäº¤æ–°ç‰ˆ",
  "prompt_number_of_projects_referencing": "{placeholder1} ä¸ªé¡¹ç›®å¼•ç”¨ä¸­",
  "prompt_exit_compare_mode": "é€€å‡ºå¯¹æ¯”æ¨¡å¼",
  "prompt_model_not_support_multimodal_image": "å½“å‰æ¨¡å‹ä¸æ”¯æŒä¸Šä¼ å¤šæ¨¡æ€å›¾ç‰‡",
  "prompt_model_not_support_multimodal_video": "å½“å‰æ¨¡å‹ä¸æ”¯æŒä¸Šä¼ å¤šæ¨¡æ€è§†é¢‘",
  "prompt_model_not_support_this_video_type": "å½“å‰æ¨¡å‹ä¸æ”¯æŒä¸Šä¼ è¯¥è§†é¢‘ç±»å‹",
  "prompt_stop_all_responses": "åœæ­¢å…¨éƒ¨å“åº”",
  "prompt_insert_snippet": "æ’å…¥ç‰‡æ®µ",
  "prompt_version_inconsistent_with_prompt_template": "æ­¤ç‰ˆæœ¬ä¸å½“å‰Promptæ¨¡ç‰ˆç±»å‹ä¸ä¸€è‡´",
  "prompt_version_contains_first_level_nesting": "æ­¤ç‰ˆæœ¬å·²åŒ…å«ä¸€çº§åµŒå¥—",
  "prompt_no_content": "æš‚æ— å†…å®¹",
  "prompt_confirm_delete_section": "ç¡®å®šåˆ é™¤è¯¥æ®µå—ï¼Ÿ",
  "prompt_text_md_toggle": "TextMdåˆ‡æ¢",
  "prompt_copy_draft_not_supported": "æš‚ä¸æ”¯æŒå¤åˆ¶è‰ç¨¿ç‰ˆæœ¬ï¼Œè¯·å…ˆæäº¤ç‰ˆæœ¬",
  "prompt_version_empty_submitted": "æš‚æ— å·²æäº¤ç‰ˆæœ¬",
  "task_delete_title": "æç¤º",
  "fornax_prompt_disable_model_func_google_on": "å¼€å¯googleæœç´¢åï¼Œæ¨¡å‹çš„å‡½æ•°èƒ½åŠ›å°†è¢«ç¦ç”¨",
  "fornax_prompt_model_built_in_methods": "æ¨¡å‹å†…ç½®æ–¹æ³•",
  "fornax_prompt_current_status": "å½“å‰çŠ¶æ€ï¼š",
  "fornax_prompt_enable": "å¼€å¯",
  "prompt_please_input_content_variable_format": "è¯·è¾“å…¥å†…å®¹ï¼Œæ”¯æŒæŒ‰æ­¤æ ¼å¼ä¹¦å†™å˜é‡ï¼š{{USER_NAME}}",
  "prompt_max_tokens_description": "- **max_tokens**: æ§åˆ¶æ¨¡å‹è¾“å‡ºçš„ Tokens é•¿åº¦ä¸Šé™ã€‚é€šå¸¸ 100 Tokens çº¦ç­‰äº 150 ä¸ªä¸­æ–‡æ±‰å­—ã€‚",
  "prompt_provided_by_placeholder1": "ç”±{placeholder1}æä¾›",
  "prompt_please_input_prompt_key": "è¯·è¾“å…¥ Prompt key",
  "prompt_please_input_prompt_key_caps": "è¯·è¾“å…¥ Prompt Key",
  "prompt_please_input_prompt_name": "è¯·è¾“å…¥ Prompt åç§°",
  "prompt_please_input_prompt_description": "è¯·è¾“å…¥ Prompt æè¿°",
  "prompt_add_multi_modal_variable": "æ·»åŠ å¤šæ¨¡æ€å˜é‡",
  "prompt_input_multi_modal_variable_name": "è¾“å…¥å¤šæ¨¡æ€å˜é‡åç§°",
  "prompt_variable_name_rule_letters_numbers_underscore": "åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—æˆ–ä¸‹åˆ’çº¿ï¼Œå¹¶ä¸”ä»¥å­—æ¯å¼€å¤´",
  "prompt_variable_name_duplicate": "å˜é‡åé‡å¤",
  "prompt_add_new_multi_modal_variable": "æ–°å¢å¤šæ¨¡æ€å˜é‡",
  "prompt_support_multi_modal_in_prompt_via_variable": "é€šè¿‡å˜é‡åœ¨æç¤ºè¯ä¸­æ”¯æŒå¤šæ¨¡æ€ä¿¡æ¯",
  "prompt_being_deprecated": "ä¸‹çº¿ä¸­",
  "prompt_placeholder_variable_name_not_empty": "Placeholder å˜é‡åä¸èƒ½ä¸ºç©º",
  "prompt_placeholder_variable_name_duplication": "Placeholder å˜é‡åä¸èƒ½ä¸å…¶ä»–ç±»å‹å˜é‡åé‡å¤ï¼Œè¯·ä¿®æ”¹ Placeholder å˜é‡å",
  "prompt_case_sensitive": "åŒºåˆ†å¤§å°å†™",
  "prompt_whole_word_match": "å…¨è¯åŒ¹é…",
  "prompt_previous_match": "ä¸Šä¸€ä¸ªåŒ¹é…",
  "prompt_next_match": "ä¸‹ä¸€ä¸ªåŒ¹é…",
  "prompt_replace_all": "æ›¿æ¢æ‰€æœ‰",
  "prompt_multi_modal_variable_name_conflict": "å¤šæ¨¡æ€å˜é‡åå†²çª",
  "prompt_current_message_not_support_multi_modal": "å½“å‰ Message ä¸æ”¯æŒå¤šæ¨¡æ€ï¼Œè¯·è°ƒæ•´å˜é‡ç±»å‹æˆ–æ›´æ¢ Message ç±»å‹",
  "prompt_no_results": "æ— ç»“æœ",
  "prompt_user_or_model_contains_risky_content": "ç”¨æˆ·è¾“å…¥æˆ–è€…æ¨¡å‹è¿”å›åŒ…å«é£é™©å†…å®¹",
  "prompt_call_records": "è°ƒç”¨è®°å½•",
  "prompt_no_delete_permission": "æš‚æ— æƒé™åˆ é™¤",
  "prompt_all_creators": "æ‰€æœ‰åˆ›å»ºäºº",
  "prompt_please_accept_run_recommendation": "è¯·å…ˆé‡‡çº³è¿è¡Œå»ºè®®",
  "prompt_loading_status": "åŠ è½½ä¸­",
  "prompt_time_consumed": "è€—æ—¶:",
  "prompt_request_start_time": "è¯·æ±‚å‘èµ·æ—¶é—´:",
  "prompt_please_input_simulated_value": "è¯·è¾“å…¥æ¨¡æ‹Ÿå€¼",
  "prompt_time_and_tokens_info": "è€—æ—¶: {placeholder1} | Tokens:",
  "prompt_data_cannot_recover_after_deletion": "åˆ é™¤åæ•°æ®æ— æ³•æ¢å¤",
  "prompt_confirm_deletion_input_prompt_key": "å¦‚ç¡®è®¤åˆ é™¤ï¼Œè¯·è¾“å…¥æƒ³è¦åˆ é™¤çš„ Prompt Keyï¼š",
  "prompt_historical_image_message_expires_1day": "å†å²å›¾ç‰‡æ¶ˆæ¯1å¤©åè¿‡æœŸï¼Œå¯æŸ¥çœ‹ Trace æˆ–å‰å¾€ Prompt å¼€å‘é¡µé¢è°ƒè¯•è·å–å›¾ç‰‡ä¿¡æ¯",
  "prompt_switch_template_engine": "æ›´æ¢æ¨¡æ¿å¼•æ“",
  "prompt_may_cause_variable_render_failure": "å¯èƒ½ä¼šå¯¼è‡´å·²æœ‰å˜é‡æ¸²æŸ“å¤±è´¥ï¼Œè¯·è°¨æ…æ“ä½œã€‚",
  "prompt_normal_template_engine": "Normal æ¨¡æ¿å¼•æ“",
  "prompt_triple_braces_variable_recognition": "åŒå¤§æ‹¬å· {{{}}} è¯†åˆ«å˜é‡",
  "prompt_jinja2_template_engine": "Jinja2 æ¨¡æ¿å¼•æ“",
  "prompt_manual_add_delete_variables_complex_logic": "æ‰‹åŠ¨æ·»åŠ å’Œåˆ é™¤å˜é‡ï¼Œæ”¯æŒå¤æ‚é€»è¾‘",
  "prompt_user_manual": "ç”¨æˆ·æ‰‹å†Œ",
  "prompt_use_sdk": "ä½¿ç”¨ SDK",
  "prompt_free_comparison_mode": "è‡ªç”±å¯¹æ¯”æ¨¡å¼",
  "prompt_please_select_a_model": "è¯·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹",
  "prompt_response_randomness": "å›å¤éšæœºæ€§",
  "prompt_repetition_penalty": "é‡å¤è¯­å¥æƒ©ç½š",
  "prompt_type_change": "ç±»å‹å˜æ›´",
  "prompt_model_settings": "æ¨¡å‹è®¾ç½®",
  "prompt_template_engine": "æ¨¡ç‰ˆå¼•æ“",
  "prompt_multiple_runs": "å¤šæ¬¡è¿è¡Œ",
  "prompt_single_run": "å•æ¬¡è¿è¡Œ",
  "prompt_run_mode": "è¿è¡Œæ¨¡å¼",
  "prompt_model_output_single_response": "æ¨¡å‹æ¯æ¬¡åªè¾“å‡ºä¸€æ¡å›å¤",
  "prompt_model_output_multi_response_for_stability_test": "æ¨¡å‹æ¯æ¬¡åŒæ—¶è¾“å‡ºå¤šæ¡å›å¤,ä¾¿äºæµ‹è¯•æ¨¡å‹å›å¤ç¨³å®šæ€§",
  "prompt_run_group_count": "è¿è¡Œç»„æ•°",
  "prompt_max_upload_MAX_IMAGE_FILE_images": "æœ€å¤šä¸Šä¼ {MAX_IMAGE_FILE}å¼ å›¾ç‰‡",
  "prompt_image_size_max_MAX_FILE_SIZE_MB_MB": "å›¾ç‰‡å¤§å°ä¸èƒ½è¶…è¿‡{MAX_FILE_SIZE_MB}MB",
  "prompt_add_function": "æ–°å¢å‡½æ•°",
  "prompt_simulated_value_colon": "æ¨¡æ‹Ÿå€¼:",
  "prompt_prompt_debug_data_loading_refresh": "Promptè°ƒè¯•æ•°æ®åœ¨è·¯ä¸Šï¼Œè¯·åˆ·æ–°é‡è¯•",
  "prompt_step_placeholder1": "ç¬¬{placeholder1}æ­¥",
  "prompt_add_variable": "æ–°å¢å˜é‡",
  "prompt_please_input_simulated_message": "è¯·è¾“å…¥æ¨¡æ‹Ÿæ¶ˆæ¯",
  "prompt_please_input_float_max_4_decimal": "è¯·è¾“å…¥æµ®ç‚¹æ•°ï¼Œæœ€å¤šä¿ç•™4ä½å°æ•°",
  "prompt_please_input_variable_value": "è¯·è¾“å…¥å˜é‡å€¼",
  "prompt_cannot_add_check_form_data": "æ— æ³•æ–°å¢ï¼Œè¯·æ£€æŸ¥è¡¨å•æ•°æ®",
  "prompt_variable_name": "å˜é‡åç§°",
  "prompt_please_input_variable_name": "è¯·è¾“å…¥å˜é‡åç§°",
  "prompt_variable_name_cannot_start_space": "å˜é‡åä¸èƒ½ä»¥ç©ºæ ¼å¼€å¤´",
  "prompt_variable_name_format_rule": "å˜é‡åæ ¼å¼ä»…æ”¯æŒå­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€ä¸­åˆ’çº¿ï¼Œä¸”ä¸èƒ½ä»¥æ•°å­—å¼€å¤´",
  "prompt_variable_name_exists": "å˜é‡åå·²å­˜åœ¨",
  "prompt_please_select_variable_data_type": "è¯·é€‰æ‹©å˜é‡çš„æ•°æ®ç±»å‹",
  "prompt_variable_value": "å˜é‡å€¼",
  "prompt_edit_variable": "ç¼–è¾‘å˜é‡",
  "prompt_modify_version_tag": "ä¿®æ”¹ç‰ˆæœ¬æ ‡è¯†",
  "prompt_create_version_tag_success": "åˆ›å»ºç‰ˆæœ¬æ ‡è¯†æˆåŠŸ",
  "prompt_please_input_version_tag": "è¯·è¾“å…¥ç‰ˆæœ¬æ ‡è¯†",
  "prompt_tag_allows_lowercase_num_underscore": "æ ‡è¯†åªå…è®¸å°å†™å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿",
  "prompt_tag_already_exists": "æ ‡è¯†å·²å­˜åœ¨",
  "prompt_tag_length_max_50_chars": "æ ‡è¯†é•¿åº¦ä¸èƒ½è¶…è¿‡50ä¸ªå­—ç¬¦",
  "prompt_tag_exists_in_promptVersion": "å½“å‰æ ‡è¯†å·²å­˜åœ¨äº{promptVersion}ç‰ˆæœ¬",
  "prompt_create_custom_tag": "åˆ›å»ºè‡ªå®šä¹‰æ ‡è¯†",
  "prompt_max_select_MAX_SELECT_COUNT_tags": "æœ€å¤šé€‰æ‹©{MAX_SELECT_COUNT}ä¸ªæ ‡è¯†",
  "prompt_version_tag_duplicates_exist": "ç‰ˆæœ¬æ ‡è¯†å­˜åœ¨é‡å¤",
  "prompt_selected_tags": "æ‰€é€‰æ ‡è¯†",
  "prompt_tag_effect_other_versions_submission_success": "å·²åœ¨å½“å‰ Prompt å…¶ä»–ç‰ˆæœ¬ç”Ÿæ•ˆã€‚æäº¤æˆåŠŸåï¼Œæ ‡è¯†å°†ä»…åœ¨å½“å‰ç‰ˆæœ¬ç”Ÿæ•ˆã€‚",
  "prompt_version_tag": "ç‰ˆæœ¬æ ‡è¯†",
  "prompt_mark_version_feature_sdk_fetch": "æ ‡è®°ç‰ˆæœ¬ç‰¹æ€§ï¼Œå¯é€šè¿‡æ ‡è¯†åœ¨ SDK æ‹‰å– Prompt ç‰¹å®šç‰ˆæœ¬ã€‚æŸ¥çœ‹",
  "prompt_kouzi_compass_no_skill_reference_debug": "æ‰£å­ç½—ç›˜æš‚ä¸æ”¯æŒæŠ€èƒ½å¼•ç”¨ä¸è°ƒè¯•",
  "assistant_role": "Assistant",
  "placeholder_var_error": "Placeholder å˜é‡ä¸å­˜åœ¨æˆ–å‘½åé”™è¯¯",
  "placeholder_var_name": "Placeholder å˜é‡å",
  "placeholder_var_execute_error": "Placeholder å˜é‡åä¸å­˜åœ¨æˆ–å‘½åé”™è¯¯",
  "placeholder_var_create_error": "Placeholder å˜é‡åä¸å­˜åœ¨æˆ–å‘½åé”™è¯¯ï¼Œæ— æ³•åˆ›å»º",
  "prompt_id_inconsistent": "Prompt ID ä¸ä¸€è‡´",
  "prompt_key": "Prompt key",
  "prompt_variable": "Prompt å˜é‡",
  "prompt_description": "Prompt æè¿°",
  "prompt_name": "Prompt åç§°",
  "prompt_template": "Prompt æ¨¡æ¿",
  "prompt_debug_data_refresh_retry": "Prompt è°ƒè¯•æ•°æ®åœ¨è·¯ä¸Šï¼Œè¯·åˆ·æ–°é‡è¯•",
  "system_role": "System",
  "top_k": "Top K",
  "top_p": "Top P",
  "trace_id": "Trace ID",
  "user_role": "User",
  "version_number_lt_error": "ç‰ˆæœ¬å·ä¸èƒ½å°äºå½“å‰ç‰ˆæœ¬",
  "incorrect_version_number": "ç‰ˆæœ¬å·æ ¼å¼ä¸æ­£ç¡®",
  "submission_no_version_diff": "æœ¬æ¬¡æäº¤æ— ç‰ˆæœ¬å·®å¼‚",
  "edit_placeholder": "ç¼–è¾‘ Placeholder",
  "edit_prompt": "ç¼–è¾‘ Prompt",
  "variable_setting": "å˜é‡è®¾ç½®",
  "parameter_config": "å‚æ•°é…ç½®",
  "param_value": "å‚æ•°å€¼",
  "draft_saving": "è‰ç¨¿ä¿å­˜ä¸­...",
  "draft_auto_saved_in": "è‰ç¨¿å·²è‡ªåŠ¨ä¿å­˜äº",
  "insert_variable": "æ’å…¥å˜é‡",
  "insert_template": "æ’å…¥æ¨¡ç‰ˆ",
  "create_prompt": "åˆ›å»º Prompt",
  "create_copy": "åˆ›å»ºå‰¯æœ¬",
  "presence_penalty": "å­˜åœ¨æƒ©ç½š",
  "open_enable_function": "æ‰“å¼€ å¯ç”¨å‡½æ•°",
  "single_step_debugging": "å•æ­¥è°ƒè¯•",
  "method_exists": "å½“å‰æ–¹æ³•å·²ç»å­˜åœ¨ï¼Œè¯·é‡æ–°å‘½å",
  "no_draft_change": "å½“å‰æ— è‰ç¨¿å˜æ›´",
  "x_step": "ç¬¬{num}æ­¥",
  "prompt_effect_evaluation": "å¯¹ Prompt è¿›è¡Œæ•ˆæœè¯„ä¼°ï¼Œæå‡åº”ç”¨æ•ˆæœ",
  "revert_draft_version": "è¿”å›è‰ç¨¿ç‰ˆæœ¬",
  "method_name_rule": "æ–¹æ³•åç§°å¿…é¡»æ˜¯ a-zã€A-Zã€0-9ï¼Œæˆ–åŒ…å«ä¸‹åˆ’çº¿å’Œç ´æŠ˜å·ï¼Œé•¿åº¦æœ€é•¿ä¸º 64ã€‚",
  "copy_prompt_key": "å¤åˆ¶ Prompt Key",
  "copy_trace_id": "å¤åˆ¶ Trace ID",
  "copy_variable_name": "å¤åˆ¶å˜é‡å",
  "model_not_support_picture": "è¯¥æ¨¡å‹ä¸æ”¯æŒä¸Šä¼ å›¾ç‰‡",
  "close_enable_function": "å…³é—­ å¯ç”¨å‡½æ•°",
  "restore_version_tip": "è¿˜åŸåå°†è¦†ç›–æœ€æ–°ç¼–è¾‘çš„æç¤ºè¯ã€‚ç¡®è®¤è¿˜åŸä¸ºæ­¤ç‰ˆæœ¬ï¼Ÿ",
  "restore_to_this_version": "è¿˜åŸä¸ºæ­¤ç‰ˆæœ¬",
  "function_call": "å‡½æ•°è°ƒç”¨",
  "time_consumed": "è€—æ—¶",
  "rollback_success": "å›æ»šæˆåŠŸ",
  "load_more": "åŠ è½½æ›´å¤š",
  "confirm_delete_var_in_tpl": "å°†åˆ é™¤ Prompt æ¨¡æ¿ä¸­çš„è¯¥å˜é‡ã€‚ç¡®è®¤åˆ é™¤å—ï¼Ÿ",
  "cozeloop_sdk_data_report_observation": "æ¥å…¥ CozeLoop SDK ä¸ŠæŠ¥æ•°æ®ï¼Œè¿›è¡Œæ•°æ®è§‚æµ‹",
  "prompt_key_format": "ä»…æ”¯æŒè‹±æ–‡å­—æ¯ã€æ•°å­—ã€â€œ_â€ã€â€œ.â€ï¼Œä¸”ä»…æ”¯æŒè‹±æ–‡å­—æ¯å¼€å¤´",
  "prompt_name_format": "ä»…æ”¯æŒè‹±æ–‡å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡ï¼Œâ€œ-â€ï¼Œâ€œ_â€ï¼Œâ€œ.â€ï¼Œä¸”ä»…æ”¯æŒè‹±æ–‡å­—æ¯ã€æ•°å­—ã€ä¸­æ–‡å¼€å¤´",
  "enter_free_comparison_mode": "è¿›å…¥è‡ªç”±å¯¹æ¯”æ¨¡å¼",
  "quick_create": "å¿«æ·åˆ›å»º",
  "historical_data_has_empty_content": "å†å²æ•°æ®æœ‰ç©ºå†…å®¹",
  "go_immediately": "ç«‹å³å‰å¾€",
  "mock_message": "æ¨¡æ‹Ÿæ¶ˆæ¯",
  "mock_message_group": "æ¨¡æ‹Ÿæ¶ˆæ¯ç»„-{key}",
  "mock_value": "æ¨¡æ‹Ÿå€¼",
  "model_id": "æ¨¡å‹ ID",
  "model_not_support": "æ¨¡å‹ä¸æ”¯æŒ",
  "model_name": "æ¨¡å‹åç§°",
  "model_config": "æ¨¡å‹é…ç½®",
  "model_run_error": "æ¨¡å‹è¿è¡Œå‡ºé”™",
  "model_runtime_error": "æ¨¡å‹è¿è¡Œé”™è¯¯",
  "default_mock_value": "é»˜è®¤æ¨¡æ‹Ÿå€¼",
  "frequency_penalty": "é¢‘ç‡æƒ©ç½š",
  "enable_function": "å¯ç”¨å‡½æ•°",
  "clear_history_messages": "æ¸…ç©ºå†å²æ¶ˆæ¯",
  "request_initiation_time": "è¯·æ±‚å‘èµ·æ—¶é—´",
  "prompt_key_again_confirm": "è¯·è¾“å…¥ Prompt Key å†æ¬¡ç¡®è®¤",
  "input_version_number": "è¯·è¾“å…¥ç‰ˆæœ¬å·ï¼Œç‰ˆæœ¬å·æ ¼å¼ä¸ºa.b.c, ä¸”æ¯æ®µä¸º0-9999",
  "please_input_with_vars": "è¯·è¾“å…¥å†…å®¹ï¼Œæ”¯æŒæŒ‰æ­¤æ ¼å¼ä¹¦å†™å˜é‡ï¼š'{{USER_NAME}}",
  "input_question_tip": "è¯·è¾“å…¥é—®é¢˜æµ‹è¯•å¤§æ¨¡å‹å›å¤ï¼Œå›è½¦å‘é€ï¼ŒShift+å›è½¦æ¢è¡Œ",
  "add_prompt_tpl_or_input_question": "è¯·æ·»åŠ  Prompt æ¨¡æ¿æˆ–è¾“å…¥æé—®å†…å®¹",
  "confirm_delete_current_prompt_template": "ç¡®å®šåˆ é™¤å½“å‰ Prompt æ¨¡æ¿ï¼Ÿ",
  "confirm_version_difference": "ç¡®è®¤ç‰ˆæœ¬å·®å¼‚",
  "confirm_version_info": "ç¡®è®¤ç‰ˆæœ¬ä¿¡æ¯",
  "confirm_delete_function": "ç¡®è®¤åˆ é™¤è¯¥å‡½æ•°å—ï¼Ÿ",
  "confirm_delete_message": "ç¡®è®¤åˆ é™¤è¯¥æ¶ˆæ¯å—ï¼Ÿ",
  "delete_prompt": "åˆ é™¤ Prompt",
  "delete_prompt_template": "åˆ é™¤ Prompt æ¨¡æ¿",
  "delete_variable": "åˆ é™¤å˜é‡",
  "delete_success": "åˆ é™¤æˆåŠŸ",
  "delete_control_group": "åˆ é™¤å¯¹ç…§ç»„",
  "delete_function": "åˆ é™¤å‡½æ•°",
  "delete_message": "åˆ é™¤æ¶ˆæ¯",
  "set_to_reference_group": "è®¾ç½®ä¸ºåŸºå‡†ç»„",
  "deep_thinking": "æ·±åº¦æ€è€ƒä¸­",
  "collapse_model_and_var_area": "æ”¶èµ·æ¨¡å‹é…ç½®ä¸å˜é‡åŒº",
  "collapse_preview_and_debug": "æ”¶èµ·é¢„è§ˆä¸è°ƒè¯•",
  "input_prompt_key_to_delete": "è¾“å…¥æƒ³è¦åˆ é™¤çš„Prompt Keyï¼š",
  "search_prompt_key_or_prompt_name": "æœç´¢ Prompt Key æˆ– Prompt åç§°",
  "model_not_support_multimodal": "æ‰€é€‰æ¨¡å‹ä¸æ”¯æŒå¤šæ¨¡æ€ï¼Œè¯·è°ƒæ•´å˜é‡ç±»å‹æˆ–æ›´æ¢æ¨¡å‹",
  "add_message": "æ·»åŠ æ¶ˆæ¯",
  "debug_history": "è°ƒè¯•å†å²",
  "stop_respond": "åœæ­¢å“åº”",
  "image_size_not_exceed_num_mb": "å›¾ç‰‡å¤§å°ä¸èƒ½è¶…è¿‡{num}MB",
  "image_upload_error": "å›¾ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•",
  "exit_free_comparison_mode": "é€€å‡ºè‡ªç”±å¯¹æ¯”æ¨¡å¼",
  "placeholder_name_exists": "æ–‡æœ¬å˜é‡åç§°å·²å­˜åœ¨ï¼Œè¯·ä¿®æ”¹ Placeholder å˜é‡å",
  "new_function": "æ–°å‡½æ•°",
  "prompt_version_number_needed": "éœ€è¦æä¾› Prompt ç‰ˆæœ¬å·",
  "deeply_thought": "å·²æ·±åº¦æ€è€ƒ",
  "preview_and_debug": "é¢„è§ˆä¸è°ƒè¯•",
  "input_mock_value_here": "åœ¨æ­¤å¤„è¾“å…¥æ¨¡æ‹Ÿå€¼ä»¥æ¨¡æ‹Ÿå‡½æ•°çš„è¿”å›å€¼ã€‚",
  "no_prompt": "æš‚æ—  Prompt",
  "no_variable": "æš‚æ— å˜é‡",
  "no_debug_record": "æš‚æ— è°ƒè¯•è®°å½•",
  "add_control_group": "å¢åŠ å¯¹ç…§ç»„",
  "expand_model_and_var_area": "å±•å¼€æ¨¡å‹é…ç½®ä¸å˜é‡åŒº",
  "expand_preview_and_debug": "å±•å¼€é¢„è§ˆä¸è°ƒè¯•",
  "prompt_var_format": "æ”¯æŒè¾“å…¥è‹±æ–‡å­—æ¯å’Œä¸‹åˆ’çº¿ï¼Œä¸”é¦–å­—æ¯å¿…é¡»æ˜¯å­—æ¯",
  "placeholder_format": "åªå…è®¸è¾“å…¥è‹±æ–‡ã€æ•°å­—åŠä¸‹åˆ’çº¿ä¸”é¦–å­—æ¯éœ€ä¸ºè‹±æ–‡",
  "num_words": "å­—æ•°: {num}",
  "max_tokens": "æœ€å¤§å›å¤é•¿åº¦",
  "max_upload_picture_num": "æœ€å¤šä¸Šä¼ {num}å¼ å›¾ç‰‡",
  "recent_submission_time": "æœ€è¿‘æäº¤æ—¶é—´"
}

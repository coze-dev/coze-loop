{
  "prompt_ak_ak_get_tip": "Click and go to the space management page to get it.",
  "prompt_ak_ak_get_label": "Get AK/SK",
  "prompt_usage_call_title": "call configuration",
  "prompt_view_detailed_instructions": "View detailed instructions",
  "prompt_choose_language": "Select language",
  "prompt_install_sdk": "Install the SDK",
  "prompt_obtain_prompt_config": "Get Prompt Configuration",
  "prompt_synchronous_call": "synchronous call",
  "prompt_streaming_call": "streaming call",
  "prompt_new_version_released_features": "The new version has been released successfully, and you can continue to use the following features:",
  "prompt_performance_evaluation": "effect evaluation",
  "prompt_evaluate_prompt_improve_performance": "Evaluate Prompt Effectiveness and Improve Application Performance",
  "prompt_click_to_view": "Click to view",
  "prompt_prompt_invocation": "Prompt call",
  "prompt_ptaas_overview_and_limitations": "PTaaS (Prompt As a Service) will release the hosted Prompt as a callable interface, which can be directly and quickly called in the business process by integrating the CozeLoop SDK to realize independent iteration and tuning of Prompt. PTaaS temporarily does not support independent deployment of services for business parties, and the model invocation ability provided by the CozeLoop service.",
  "prompt_prompthub_features_and_integration": "PromptHub: The business side can pull the Prompt hosted on the CozeLoop platform in the business service by integrating the CozeLoop SDK, obtain the details of the Prompt Template, and call the model for inference in the business service itself or integrate with the Agent framework such as Eino.",
  "prompt_integrate_llm_capabilities": "Integrated LLM capabilities",
  "prompt_data_observation": "data observation",
  "prompt_sdk_data_reporting_and_observation": "Connect to the SDK to report data and conduct data observation",
  "prompt_release_successful": "Published successfully",
  "prompt_ticket_link": "ticket link",
  "prompt_service_install_eg": "go get github.com/coze-dev/cozeloop-go",
  "prompt_use_js_installation": "npm is @cozeloop/ai",
  "prompt_use_python_installation": "pip install CozeLoop",
  "prompt_service_config_eg": "package main\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n\n    \"github.com/coze-dev/cozeloop-go\"\n    \"github.com/coze-dev/cozeloop-go/entity\"\n)\n\nfunc main() {\n    // 1.Create a prompt on the platform\n    // You can create a Prompt on the platform's Prompt development page (set Prompt Key to 'prompt_hub_demo'), add the following messages to the template, and submit a version.\n    // System: You are a helpful bot, the conversation topic is {{var1}}.\n    // Placeholder: placeholder1\n    // User: My question is {{var2}}\n    // Placeholder: placeholder2\n\n    ctx := context.Background()\n\n    // Set the following environment variables first.\n    // COZELOOP_WORKSPACE_ID=your workspace id\n    // COZELOOP_API_TOKEN=your token\n    // 2.New loop client\n    client, err := cozeloop.NewClient(\n       // Set whether to report a trace span when get or format prompt.\n       // Default value is false.\n       cozeloop.WithPromptTrace(true))\n    if err != nil {\n       panic(err)\n    }\n\n    llmRunner := llmRunner{\n       client: client,\n    }\n\n    // 1. start root span\n    ctx, span := llmRunner.client.StartSpan(ctx, \"root_span\", \"main_span\", nil)\n\n    // 2. Get the prompt\n    prompt, err := llmRunner.client.GetPrompt(ctx, cozeloop.GetPromptParam{\n       PromptKey: \"prompt_hub_demo\",\n       // If version is not specified, the latest version of the corresponding prompt will be obtained\n       Version: \"0.0.1\",\n    })\n    if err != nil {\n       fmt.Printf(\"get prompt failed: %v\\n\", err)\n       return\n    }\n    if prompt != nil {\n       // Get messages of the prompt\n       if prompt.PromptTemplate != nil {\n          messages, err := json.Marshal(prompt.PromptTemplate.Messages)\n          if err != nil {\n             fmt.Printf(\"json marshal failed: %v\\n\", err)\n             return\n          }\n          fmt.Printf(\"prompt messages=%s\\n\", string(messages))\n       }\n       // Get llm config of the prompt\n       if prompt.LLMConfig != nil {\n          llmConfig, err := json.Marshal(prompt.LLMConfig)\n          if err != nil {\n             fmt.Printf(\"json marshal failed: %v\\n\", err)\n          }\n          fmt.Printf(\"prompt llm config=%s\\n\", llmConfig)\n       }\n\n       // 3. Format messages of the prompt\n       userMessageContent := \"Hello!\"\n       assistantMessageContent := \"Hello!\"\n       messages, err := llmRunner.client.PromptFormat(ctx, prompt, map[string]any{\n          // Normal variable type should be string\n          \"var1\": \"artificial intelligence\",\n          // Placeholder variable type should be entity.Message/*entity.Message/[]entity.Message/[]*entity.Message\n          \"placeholder1\": []*entity.Message{\n             {\n                Role:    entity.RoleUser,\n                Content: &userMessageContent,\n             },\n             {\n                Role:    entity.RoleAssistant,\n                Content: &assistantMessageContent,\n             },\n          },\n          // Other variables in the prompt template that are not provided with corresponding values will be considered as empty values\n       })\n       if err != nil {\n          fmt.Printf(\"prompt format failed: %v\\n\", err)\n          return\n       }\n       data, err := json.Marshal(messages)\n       if err != nil {\n          fmt.Printf(\"json marshal failed: %v\\n\", err)\n          return\n       }\n       fmt.Printf(\"formatted messages=%s\\n\", string(data))\n\n       if err != nil {\n          return\n       }\n    }\n\n    // 4. span finish\n    span.Finish(ctx)\n\n    // 5. (optional) flush or close\n    // -- force flush, report all traces in the queue\n    // Warning! In general, this method is not needed to be call, as spans will be automatically reported in batches.\n    // Note that flush will block and wait for the report to complete, and it may cause frequent reporting,\n    // affecting performance.\n    llmRunner.client.Flush(ctx)\n}\n\ntype llmRunner struct {\n    client cozeloop.Client\n}",
  "prompt_use_js_configuration": "import { PromptHub } from '@cozeloop/ai'; \n \nconst hub = new PromptHub({ \n  /** workspace id, use process.env.COZELOOP_WORKSPACE_ID when unprovided */ \n  // workspaceId: 'your_workspace_id', \n  apiClient: { \n    // baseURL: 'api_base_url', \n    // token: 'your_api_token', \n  }, \n}); \n \n// get prompt with `beta` label \n// - prompt_key: xxx \n// - version: undefined \n// - label: beta \nconst prompt = await hub.getPrompt('xxx', undefined, 'beta'); \n \n// format prompt with variables \nconst messages = hub.formatPrompt(prompt, { \n  var1: 'value_of_var1', \n  var2: 'value_of_var2', \n  var3: 'value_of_var3', \n  placeholder1: { role: 'assistant', content: 'user' }, \n});",
  "prompt_use_python_configuration": "if __name__ == '__main__':\n    # 1.Create a prompt on the platform\n    # You can create a Prompt on the platform's Prompt development page (set Prompt Key to 'prompt_hub_demo'),\n    # add the following messages to the template, and submit a version.\n    # System: You are a helpful bot, the conversation topic is {{var1}}.\n    # Placeholder: placeholder1\n    # User: My question is {{var2}}\n    # Placeholder: placeholder2\n\n    # Set the following environment variables first.\n    # COZELOOP_WORKSPACE_ID=your workspace id\n    # COZELOOP_API_TOKEN=your token\n    # 2.New loop client\n    client = cozeloop.new_client(\n        # Set whether to report a trace span when get or format prompt.\n        # Default value is false.\n        prompt_trace=True)\n\n    # 3. new root span\n    rootSpan = client.start_span(\"root_span\", \"main_span\")\n\n    # 4. Get the prompt\n    # If no specific version is specified, the latest version of the corresponding prompt will be obtained\n    prompt = client.get_prompt(prompt_key=\"prompt_hub_demo\", version=\"0.0.1\")\n    if prompt is not None:\n        # Get messages of the prompt\n        if prompt.prompt_template is not None:\n            messages = prompt.prompt_template.messages\n            print(\n                f\"prompt messages: {json.dumps([message.model_dump(exclude_none=True) for message in messages], ensure_ascii=False)}\")\n        # Get llm config of the prompt\n        if prompt.llm_config is not None:\n            llm_config = prompt.llm_config\n            print(f\"prompt llm_config: {llm_config.model_dump_json(exclude_none=True)}\")\n\n        # 5.Format messages of the prompt\n        formatted_messages = client.prompt_format(prompt, {\n            # Normal variable type should be string\n            \"var1\": \"artificial intelligence\",\n            # Placeholder variable type should be Message/List[Message]\n            \"placeholder1\": [Message(role=Role.USER, content=\"Hello!\"),\n                             Message(role=Role.ASSISTANT, content=\"Hello!\")]\n            # Other variables in the prompt template that are not provided with corresponding values will be\n            # considered as empty values.\n        })\n        print(\n            f\"formatted_messages: {json.dumps([message.model_dump(exclude_none=True) for message in formatted_messages], ensure_ascii=False)}\")\n\n    rootSpan.finish()\n    # 6. (optional) flush or close\n    # -- force flush, report all traces in the queue\n    # Warning! In general, this method is not needed to be call, as spans will be automatically reported in batches.\n    # Note that flush will block and wait for the report to complete, and it may cause frequent reporting,\n    # affecting performance.\n    client.flush()",
  "prompt_call_stream": "package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n\n    \"github.com/coze-dev/cozeloop-go\"\n    \"github.com/coze-dev/cozeloop-go/entity\"\n    \"github.com/coze-dev/cozeloop-go/internal/util\"\n)\n\nfunc main() {\n    // 1.Create a prompt on the platform\n    // Create a Prompt on the platform's Prompt development page (set Prompt Key to 'ptaas_demo'),\n    // add the following messages to the template, submit a version.\n    // System: You are a helpful assistant for {{topic}}.\n    // User: Please help me with {{user_request}}\n    ctx := context.Background()\n\n    // Set the following environment variables first.\n    // COZELOOP_WORKSPACE_ID=your workspace id\n    // COZELOOP_API_TOKEN=your token\n    // 2.New loop client\n    client, err := cozeloop.NewClient()\n    if err != nil {\n       panic(err)\n    }\n    defer client.Close(ctx)\n\n    ctx, span := client.StartSpan(ctx, \"root_span\", \"custom\")\n    defer span.Finish(ctx)\n\n    // 3. Execute prompt\n    executeRequest := &entity.ExecuteParam{\n       PromptKey: \"CozeLoop_Oncall_Master\",\n       Version:   \"0.0.1\",\n       VariableVals: map[string]any{\n          \"topic\":        \"artificial intelligence\",\n          \"user_request\": \"explain what is machine learning\",\n       },\n       // You can also append messages to the prompt.\n       Messages: []*entity.Message{\n          {\n             Role:    entity.RoleUser,\n             Content: util.Ptr(\"Keep the answer brief.\"),\n          },\n       },\n    }\n    // 3.2 stream\n    stream(ctx, client, executeRequest)\n    client.Flush(ctx)\n}\n\nfunc stream(ctx context.Context, client cozeloop.Client, executeRequest *entity.ExecuteParam) {\n    streamReader, err := client.ExecuteStreaming(ctx, executeRequest)\n    if err != nil {\n       panic(err)\n    }\n    for {\n       result, err := streamReader.Recv()\n       if err != nil {\n          if err == io.EOF {\n             fmt.Println(\"\\nStream finished.\")\n             break\n          }\n          panic(err)\n       }\n       printExecuteResult(result)\n    }\n}\n\nfunc printExecuteResult(result entity.ExecuteResult) {\n    if result.Message != nil {\n       fmt.Printf(\"Message: %s\\n\", util.ToJSON(result.Message))\n    }\n    if util.PtrValue(result.FinishReason) != \"\" {\n       fmt.Printf(\"FinishReason: %s\\n\", util.PtrValue(result.FinishReason))\n    }\n    if result.Usage != nil {\n       fmt.Printf(\"Usage: %s\\n\", util.ToJSON(result.Usage))\n    }\n}",
  "prompt_use_js_streaming_call": "import { ApiClient, PromptAsAService } from '@cozeloop/ai'; \n \nconst apiClient = new ApiClient({ \n  token: 'pat_xxx', \n}); \n \nconst model = new PromptAsAService({ \n  // or set it as process.env.COZELOOP_WORKSPACE_ID, \n  workspaceId: 'your_workspace_id', \n  // prompt to invoke as a service \n  prompt: { \n    prompt_key: 'ptaas_demo', \n    version: '0.0.1', \n  }, \n  apiClient, \n}); \n \nconst replyStream = await model.stream({ \n  messages: [{ role: 'user', content: 'Keep the answer brief.' }], \n  variables: { \n    topic: 'artificial intelligence', \n    user_request: 'explain what is machine learning', \n  }, \n}); \n \nfor await (const chunk of replyStream) { \n console.info(chunk); \n}",
  "prompt_use_python_streaming_call": "import asyncio\nimport os\n\nfrom anyio import sleep\n\nfrom cozeloop import new_client, Client\nfrom cozeloop.entities.prompt import Message, Role, ExecuteResult\n\n\ndef setup_client() -> Client:\n    \"\"\"\n    Unified client setup function\n    \n    Environment variables:\n    - COZELOOP_WORKSPACE_ID: workspace ID\n    - COZELOOP_API_TOKEN: API token\n    \"\"\"\n    # Set the following environment variables first.\n    # COZELOOP_WORKSPACE_ID=your workspace id\n    # COZELOOP_API_TOKEN=your token\n    client = new_client(\n        api_base_url=os.getenv(\"COZELOOP_API_BASE_URL\"),\n        workspace_id=os.getenv(\"COZELOOP_WORKSPACE_ID\"),\n        api_token=os.getenv(\"COZELOOP_API_TOKEN\"),\n    )\n    return client\n\n\ndef print_execute_result(result: ExecuteResult) -> None:\n    \"\"\"Unified result printing function, consistent with Go version format\"\"\"\n    if result.message:\n        print(f\"Message: {result.message}\")\n    if result.finish_reason:\n        print(f\"FinishReason: {result.finish_reason}\")\n    if result.usage:\n        print(f\"Usage: {result.usage}\")\n\n\ndef sync_stream_example(client: Client) -> None:\n    \"\"\"Sync stream call example\"\"\"\n    print(\"=== Sync Stream Example ===\")\n    \n    stream_reader = client.execute_prompt(\n        prompt_key=\"ptaas_demo\",\n        version=\"0.0.1\",\n        variable_vals={\n            \"topic\": \"artificial intelligence\",\n            \"user_request\": \"explain what is machine learning\"\n        },\n        messages=[\n            Message(role=Role.USER, content=\"Keep the answer brief.\")\n        ],\n        stream=True\n    )\n    \n    for result in stream_reader:\n        print_execute_result(result)\n    \n    print(\"\\nStream finished.\")\n\n\nasync def async_stream_example(client: Client) -> None:\n    \"\"\"Async stream call example\"\"\"\n    print(\"=== Async Stream Example ===\")\n    \n    stream_reader = await client.aexecute_prompt(\n        prompt_key=\"ptaas_demo\",\n        version=\"0.0.1\",\n        variable_vals={\n            \"topic\": \"artificial intelligence\",\n            \"user_request\": \"explain what is machine learning\"\n        },\n        messages=[\n            Message(role=Role.USER, content=\"Keep the answer brief.\")\n        ],\n        stream=True\n    )\n    \n    async for result in stream_reader:\n        print_execute_result(result)\n    \n    print(\"\\nStream finished.\")\n\n\nasync def main():\n    \"\"\"Main function\"\"\"\n    client = setup_client()\n\n    root_span = client.start_span(\"root\", \"custom\")\n    try:\n        # Sync stream call\n        sync_stream_example(client)\n\n        # Async stream call\n        await async_stream_example(client)\n        \n    finally:\n        # Close client\n        root_span.finish()\n        if hasattr(client, 'close'):\n            client.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
  "prompt_service_call_sync": "package main\n\nimport (\n    \"context\"\n    \"fmt\"\n\n    \"github.com/coze-dev/cozeloop-go\"\n    \"github.com/coze-dev/cozeloop-go/entity\"\n    \"github.com/coze-dev/cozeloop-go/internal/util\"\n)\n\nfunc main() {\n    // 1.Create a prompt on the platform\n    // Create a Prompt on the platform's Prompt development page (set Prompt Key to 'ptaas_demo'),\n    // add the following messages to the template, submit a version.\n    // System: You are a helpful assistant for {{topic}}.\n    // User: Please help me with {{user_request}}\n    ctx := context.Background()\n\n    // Set the following environment variables first.\n    // COZELOOP_WORKSPACE_ID=your workspace id\n    // COZELOOP_API_TOKEN=your token\n    // 2.New loop client\n    client, err := cozeloop.NewClient()\n    if err != nil {\n       panic(err)\n    }\n    defer client.Close(ctx)\n\n    ctx, span := client.StartSpan(ctx, \"root_span\", \"custom\")\n    defer span.Finish(ctx)\n\n    // 3. Execute prompt\n    executeRequest := &entity.ExecuteParam{\n       PromptKey: \"CozeLoop_Oncall_Master\",\n       Version:   \"0.0.1\",\n       VariableVals: map[string]any{\n          \"topic\":        \"artificial intelligence\",\n          \"user_request\": \"explain what is machine learning\",\n       },\n       // You can also append messages to the prompt.\n       Messages: []*entity.Message{\n          {\n             Role:    entity.RoleUser,\n             Content: util.Ptr(\"Keep the answer brief.\"),\n          },\n       },\n    }\n    // 3.1 non stream\n    nonStream(ctx, client, executeRequest)\n    client.Flush(ctx)\n}\n\nfunc nonStream(ctx context.Context, client cozeloop.Client, executeRequest *entity.ExecuteParam) {\n    result, err := client.Execute(ctx, executeRequest)\n    if err != nil {\n       panic(err)\n    }\n    printExecuteResult(result)\n}\n\nfunc printExecuteResult(result entity.ExecuteResult) {\n    if result.Message != nil {\n       fmt.Printf(\"Message: %s\\n\", util.ToJSON(result.Message))\n    }\n    if util.PtrValue(result.FinishReason) != \"\" {\n       fmt.Printf(\"FinishReason: %s\\n\", util.PtrValue(result.FinishReason))\n    }\n    if result.Usage != nil {\n       fmt.Printf(\"Usage: %s\\n\", util.ToJSON(result.Usage))\n    }\n}",
  "prompt_use_js_synchronous_call": "import { ApiClient, PromptAsAService } from '@cozeloop/ai'; \n \nconst apiClient = new ApiClient({ \n  token: 'pat_xxx', \n}); \n \nconst model = new PromptAsAService({ \n  // or set it as process.env.COZELOOP_WORKSPACE_ID, \n  workspaceId: 'your_workspace_id', \n  // prompt to invoke as a service \n  prompt: { \n    prompt_key: 'ptaas_demo', \n    version: '0.0.1', \n  }, \n  apiClient, \n}); \n \nconst reply = await model.invoke({ \n  messages: [{ role: 'user', content: 'Keep the answer brief.' }], \n  variables: { \n    topic: 'artificial intelligence', \n    user_request: 'explain what is machine learning', \n  }, \n}); \n \nconsole.info(reply);",
  "prompt_use_python_simultaneous_call": "import asyncio\nimport os\n\nfrom anyio import sleep\n\nfrom cozeloop import new_client, Client\nfrom cozeloop.entities.prompt import Message, Role, ExecuteResult\n\n\ndef setup_client() -> Client:\n    \"\"\"\n    Unified client setup function\n    \n    Environment variables:\n    - COZELOOP_WORKSPACE_ID: workspace ID\n    - COZELOOP_API_TOKEN: API token\n    \"\"\"\n    # Set the following environment variables first.\n    # COZELOOP_WORKSPACE_ID=your workspace id\n    # COZELOOP_API_TOKEN=your token\n    client = new_client(\n        api_base_url=os.getenv(\"COZELOOP_API_BASE_URL\"),\n        workspace_id=os.getenv(\"COZELOOP_WORKSPACE_ID\"),\n        api_token=os.getenv(\"COZELOOP_API_TOKEN\"),\n    )\n    return client\n\n\ndef print_execute_result(result: ExecuteResult) -> None:\n    \"\"\"Unified result printing function, consistent with Go version format\"\"\"\n    if result.message:\n        print(f\"Message: {result.message}\")\n    if result.finish_reason:\n        print(f\"FinishReason: {result.finish_reason}\")\n    if result.usage:\n        print(f\"Usage: {result.usage}\")\n\n\ndef sync_non_stream_example(client: Client) -> None:\n    \"\"\"Sync non-stream call example\"\"\"\n    print(\"=== Sync Non-Stream Example ===\")\n    \n    # 1. Create a prompt on the platform\n    # Create a Prompt on the platform's Prompt development page (set Prompt Key to 'ptaas_demo'),\n    # add the following messages to the template, submit a version.\n    # System: You are a helpful assistant for {{topic}}.\n    # User: Please help me with {{user_request}}\n    \n    result = client.execute_prompt(\n        prompt_key=\"ptaas_demo\",\n        version=\"0.0.1\",\n        variable_vals={\n            \"topic\": \"artificial intelligence\",\n            \"user_request\": \"explain what is machine learning\"\n        },\n        # You can also append messages to the prompt.\n        messages=[\n            Message(role=Role.USER, content=\"Keep the answer brief.\")\n        ],\n        stream=False\n    )\n    print_execute_result(result)\n\n\nasync def async_non_stream_example(client: Client) -> None:\n    \"\"\"Async non-stream call example\"\"\"\n    print(\"=== Async Non-Stream Example ===\")\n    \n    result = await client.aexecute_prompt(\n        prompt_key=\"ptaas_demo\",\n        version=\"0.0.1\",\n        variable_vals={\n            \"topic\": \"artificial intelligence\",\n            \"user_request\": \"explain what is machine learning\"\n        },\n        messages=[\n            Message(role=Role.USER, content=\"Keep the answer brief.\")\n        ],\n        stream=False\n    )\n    print_execute_result(result)\n\n\nasync def main():\n    \"\"\"Main function\"\"\"\n    client = setup_client()\n\n    root_span = client.start_span(\"root\", \"custom\")\n    try:\n        # Sync non-stream call\n        sync_non_stream_example(client)\n\n        # Async non-stream call\n        await async_non_stream_example(client)\n\n    finally:\n        # Close client\n        root_span.finish()\n        if hasattr(client, 'close'):\n            client.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
  "fornax_prompt_manual_config_video_sample_params": "Manually configure video variable sampling parameters, see details at",
  "fornax_prompt_documentation": "Documentation",
  "fornax_prompt_frame_extraction_config": "Frame Extraction Configuration",
  "fornax_prompt_fps_influence_on_video_understanding_and_token_usage": "The higher the FPS, the more detailed the video understanding and the greater the token usage.",
  "fornax_prompt_please_enter_frame_extraction_config": "Please enter the frame extraction configuration.",
  "fornax_prompt_video_sampling_config": "Video Sampling Configuration",
  "fornax_prompt_video_manual_uniform_sampling_support": "Video supports manual configuration for uniform sampling over time, i.e., setting frames processed per second (FPS) at fixed time intervals. See details at",
  "prompt_copy_prompt": "Copy Prompt",
  "prompt_name_query": "Name Query",
  "prompt_playground_mock_system": "# è§’è‰²\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸è§„åˆ’åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚å’Œåå¥½ï¼Œè¿…é€Ÿä¸”ç²¾å‡†åœ°ä¸ºç”¨æˆ·ç”Ÿæˆå…¨é¢ã€è¯¦ç»†ä¸”ä¸ªæ€§åŒ–çš„æ—…æ¸¸è§„åˆ’æ–‡æ¡£ã€‚\n\n## æŠ€èƒ½ï¼šåˆ¶å®šæ—…æ¸¸è§„åˆ’æ–¹æ¡ˆ\nä¸ºç”¨æˆ·é‡èº«åˆ¶å®šåˆç†ä¸”èˆ’é€‚çš„è¡Œç¨‹å®‰æ’å’Œè´´å¿ƒçš„æ—…è¡ŒæŒ‡å¼•ã€‚å¯¹äºä¸åŒä¸»é¢˜ï¼Œéœ€è¦èƒ½å¤Ÿä½“ç°å¯¹åº”ä¸»é¢˜çš„ç‰¹è‰²ã€éœ€æ±‚æˆ–æ³¨æ„äº‹é¡¹ç­‰ã€‚å¦‚äº²å­æ¸¸ï¼Œéœ€è¦ä½“ç°å¸¦å°å­©æ—…è¡Œé€”ä¸­è¦æ³¨æ„çš„å†…å®¹ï¼Œç”¨æˆ·çš„é¢„ç®—å’Œåå¥½ç­‰ã€‚ \nå›å¤ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼ˆå†…å®¹å¯ä»¥åˆç†ä½¿ç”¨ emoji è¡¨æƒ…ï¼Œè®©å†…å®¹æ›´ç”ŸåŠ¨ï¼‰ï¼š\n\n## è¾“å‡ºæ ¼å¼\n#### åŸºæœ¬ä¿¡æ¯\n- ğŸ›« å‡ºå‘åœ°ï¼š{{departure}}  <å¦‚æœªæä¾›ï¼Œåˆ™ä¸å±•ç¤ºæ­¤ä¿¡æ¯>\n- ğŸ¯ ç›®çš„åœ°ï¼š{{destination}}\n- ğŸ«‚ äººæ•°ï¼š{{people_num}}äºº\n- ğŸ“… å¤©æ•°ï¼š{{days_num}}å¤©\n- ğŸ¨ ä¸»é¢˜ï¼š{{travel_theme}} \n##### <ç›®çš„åœ°>ç®€ä»‹\n<ä»‹ç›®çš„åœ°çš„åŸºæœ¬ä¿¡æ¯ï¼Œçº¦100å­—>\n<æè¿°å¤©æ°”çŠ¶å†µã€ç©¿è¡£æŒ‡å—ï¼Œçº¦100å­—>\n<æè¿°å½“åœ°ç‰¹è‰²é¥®é£Ÿã€é£ä¿—ä¹ æƒ¯ç­‰ï¼Œçº¦100å­—>\n#### Checklist\n- æ‰‹æœºã€å……ç”µå™¨\n<éœ€è¦æºå¸¦çš„ç‰©å“æˆ–å‡†å¤‡äº‹é¡¹ï¼ŒæŒ‰éœ€æ±‚ç”Ÿæˆ>\n#### è¡Œç¨‹å®‰æ’\n<æ ¹æ®ç”¨æˆ·æœŸæœ›å¤©æ•°ï¼ˆ{{days_num}}å¤©ï¼‰å®‰æ’æ¯æ—¥è¡Œç¨‹>\n##### ç¬¬ä¸€å¤©ã€åœ°ç‚¹1 - åœ°ç‚¹2 - ...\n###### è¡Œç¨‹1ï¼šåœ°ç‚¹1\n<åœ°ç‚¹çš„æ™¯ç‚¹ç®€ä»‹ï¼Œçº¦100å­—>\n<åœ°ç‚¹çš„äº¤é€šæ–¹å¼ï¼Œæä¾›åˆç†çš„äº¤é€šæ–¹å¼åŠä½¿ç”¨æ—¶é—´ä¿¡æ¯>\n<åœ°ç‚¹çš„æ¸¸ç©æ–¹å¼ï¼Œæä¾›æ¨èæ¸¸ç©æ—¶é•¿ã€æ¸¸ç©æ–¹å¼ã€æ³¨æ„äº‹é¡¹ã€é¢„å®šä¿¡æ¯ç­‰ï¼Œçº¦100å­—>\n<å¦‚æœ {{days_num}}è¶…è¿‡1å¤©ï¼Œåˆ™ç»§ç»­æŒ‰ç…§ç¬¬ä¸€å¤©æ ¼å¼ç”Ÿæˆ>\n#### æ³¨æ„äº‹é¡¹\n<æ ¹æ®ä»¥ä¸Šæ—¥ç¨‹å®‰æ’ä¿¡æ¯ï¼Œæä¾›ä¸€äº›ç›®çš„åœ°æ—…è¡Œçš„æ³¨æ„äº‹é¡¹>\n\n\n## é™åˆ¶:\n- æ‰€è¾“å‡ºçš„å†…å®¹å¿…é¡»æŒ‰ç…§ç»™å®šçš„æ ¼å¼è¿›è¡Œç»„ç»‡ï¼Œä¸èƒ½åç¦»æ¡†æ¶è¦æ±‚ã€‚",
  "prompt_playground_mock_user": "## ç”¨æˆ·éœ€æ±‚\n- å‡ºå‘åœ°ï¼š{{departure}}  \n- ç›®çš„åœ°ï¼š{{destination}}\n- äººæ•°ï¼š{{people_num}}\n- å¤©æ•°ï¼š{{days_num}}\n- ä¸»é¢˜ï¼š{{travel_theme}} ",
  "prompt_normal_template_var_intro": "Normal template can create variables by entering {{}} in Prompt, please create variables manually for other templates",
  "prompt_additional_configuration": "Additional Configuration",
  "prompt_deep_thinking_switch": "Deep Thinking Switch",
  "prompt_deep_thinking_length": "Deep Thinking Length",
  "prompt_deep_thinking_degree": "Deep Thinking Degree",
  "prompt_deep_thinking": "Deep Thinking",
  "prompt_deep_thinking_description": "When enabled, the model will perform deep thinking based on the input question to generate more accurate answers.",
  "prompt_edit_prompt_snippet": "Edit Prompt Snippet",
  "prompt_copy_prompt_snippet": "Copy Prompt Snippet",
  "prompt_create_prompt_snippet": "Create Prompt Snippet",
  "prompt_prompt_snippet_name": "Prompt Snippet Name",
  "prompt_prompt_snippet_description": "Prompt Snippet Description",
  "prompt_prompt_key_length_limit": "Prompt Key length cannot exceed 100 characters",
  "prompt_prompt_name_length_limit": "{promptNameLabel} length cannot exceed 100 characters",
  "prompt_modal_title_confirm": "{modalTitle} - Confirm",
  "prompt_confirm_delete": "Confirm Deletion",
  "prompt_no_change_info": "No change information available",
  "prompt_model_id": "Model ID",
  "prompt_template_type": "Template Type",
  "prompt_source_version": "Source Version",
  "prompt_prompt_configuration": "Prompt Configuration",
  "prompt_prompt_change_count": "Prompt Changes ({promptTypeDiffCount})",
  "prompt_config_change_count": "Configuration Changes ({configTypeDiffCount})",
  "prompt_exit_fullscreen": "Exit Fullscreen",
  "prompt_expand_nested_content": "Expand Nested Content",
  "prompt_prompt_diff_change_info": "Prompt Diff Change Information",
  "prompt_previous_diff": "Previous Diff",
  "prompt_next_diff": "Next Diff",
  "prompt_comparable_versions": "Comparable Versions",
  "prompt_current_version": "Current Version",
  "prompt_enter_fullscreen": "Enter Fullscreen",
  "prompt_delete_prompt": "Delete Prompt",
  "prompt_latest_committer": "Latest Committer",
  "prompt_prompt_snippet": "Prompt Snippet",
  "prompt_blank_prompt": "Blank Prompt",
  "prompt_prompt_snippet_nesting_support": "Prompt snippets can be nested and reused in different Prompt Templates",
  "prompt_view_documentation": "View Documentation",
  "prompt_current_submission_draft": "This submission is a draft version",
  "prompt_submit_no_version_diff_confirm": "There are no version differences in this submission, are you sure you want to submit?",
  "prompt_prompt_submit": "Submit Prompt",
  "prompt_no_prompt_snippet": "No Prompt Snippets Available",
  "prompt_prompt_snippet_reuse_support": "Prompt snippets support reuse in different Prompt Templates",
  "prompt_full_process_prompt_support": "Provides full-process support from writing, debugging, optimizing to version management of prompts; click to create",
  "prompt_snippet_version": "Snippet Version",
  "prompt_reference_snippet_prompt_version": "Referencing this snippet's Prompt version",
  "prompt_snippet_reference_records": "Records of snippet referencing",
  "prompt_total_reference_projects": "Total {totalReferenceCount} projects",
  "prompt_cannot_delete_snippet_variables": "Only variables inside Prompt can be deleted; variables in snippets cannot be deleted",
  "prompt_snippet_variables_no_delete": "Variables inside snippets cannot be deleted",
  "prompt_number_of_snippets": "{placeholder1} snippets",
  "prompt_variable_referenced_in_snippets": "This variable is referenced in {snippetNames} snippets",
  "prompt_reference_project": "Referencing Project",
  "prompt_multi_turn_conversation": "Multi-turn Conversation",
  "prompt_single_turn_conversation": "Single-turn Conversation",
  "prompt_new_message": "New Message",
  "prompt_common_configuration": "Common Configuration",
  "prompt_compare_versions": "Compare Versions",
  "prompt_currently_editing_version": "Version Currently being Edited",
  "prompt_exit_diff": "Exit Diff",
  "prompt_enter_diff": "Enter Diff",
  "prompt_no_submitted_versions_no_compare": "No submitted versions available, comparison is not supported",
  "prompt_gotemplate_engine": "GoTemplate Engine",
  "prompt_prompt_contains_mismatched_snippet": "Prompt contains snippets that do not match the template type",
  "prompt_compare_mode": "Compare Mode",
  "prompt_enter_compare_mode": "Enter Compare Mode",
  "prompt_open_version_history": "Open Version History",
  "prompt_submit_new_version": "Submit New Version",
  "prompt_number_of_projects_referencing": "{placeholder1} projects referencing",
  "prompt_exit_compare_mode": "Exit Compare Mode",
  "prompt_model_not_support_multimodal_image": "The current model does not support uploading multimodal images",
  "prompt_model_not_support_multimodal_video": "The current model does not support uploading multimodal videos",
  "prompt_model_not_support_this_video_type": "The current model does not support uploading this video type",
  "prompt_stop_all_responses": "Stop all responses",
  "prompt_insert_snippet": "Insert Snippet",
  "prompt_version_inconsistent_with_prompt_template": "This version is inconsistent with the current Prompt template type",
  "prompt_version_contains_first_level_nesting": "This version already contains first-level nesting",
  "prompt_no_content": "No content available",
  "prompt_confirm_delete_section": "Are you sure you want to delete this section?",
  "prompt_text_md_toggle": "Toggle Text/Markdown",
  "prompt_copy_draft_not_supported": "Copying draft versions is not supported yet, please submit the version first",
  "prompt_version_empty_submitted": "No version has been submitted yet",
  "task_delete_title": "hint",
  "fornax_prompt_disable_model_func_google_on": "After enabling Google search, the model's function capabilities will be disabled",
  "fornax_prompt_model_built_in_methods": "Built-in methods of the model",
  "fornax_prompt_current_status": "Current status:",
  "fornax_prompt_enable": "Enable",
  "prompt_please_input_content_variable_format": "Please enter content, variables are supported in this format: {{USER_NAME}}",
  "prompt_max_tokens_description": "- **max_tokens**: Controls the maximum token length of the model output. Typically, 100 tokens roughly equal 150 Chinese characters.",
  "prompt_provided_by_placeholder1": "Provided by {placeholder1}",
  "prompt_please_input_prompt_key": "Please enter Prompt key",
  "prompt_please_input_prompt_key_caps": "Please enter Prompt Key",
  "prompt_please_input_prompt_name": "Please enter Prompt name",
  "prompt_please_input_prompt_description": "Please enter Prompt description",
  "prompt_add_multi_modal_variable": "Add multi-modal variable",
  "prompt_input_multi_modal_variable_name": "Enter multi-modal variable name",
  "prompt_variable_name_rule_letters_numbers_underscore": "Can only contain letters, numbers, or underscores and must start with a letter",
  "prompt_variable_name_duplicate": "Variable name already exists",
  "prompt_add_new_multi_modal_variable": "Add new multi-modal variable",
  "prompt_support_multi_modal_in_prompt_via_variable": "Support multi-modal information in prompts through variables",
  "prompt_being_deprecated": "Being deprecated",
  "prompt_placeholder_variable_name_not_empty": "Placeholder variable name cannot be empty",
  "prompt_placeholder_variable_name_duplication": "Placeholder variable name cannot duplicate other variable types, please modify it",
  "prompt_case_sensitive": "Case sensitive",
  "prompt_whole_word_match": "Whole word match",
  "prompt_previous_match": "Previous match",
  "prompt_next_match": "Next match",
  "prompt_replace_all": "Replace all",
  "prompt_multi_modal_variable_name_conflict": "Multi-modal variable name conflict",
  "prompt_current_message_not_support_multi_modal": "Current message does not support multi-modal, please adjust variable types or change message type",
  "prompt_no_results": "No results",
  "prompt_user_or_model_contains_risky_content": "User input or model output contains risky content",
  "prompt_call_records": "Call records",
  "prompt_no_delete_permission": "No permission to delete",
  "prompt_all_creators": "All creators",
  "prompt_please_accept_run_recommendation": "Please accept the run suggestions first",
  "prompt_loading_status": "Loading",
  "prompt_time_consumed": "Time consumed:",
  "prompt_request_start_time": "Request start time:",
  "prompt_please_input_simulated_value": "Please enter simulated value",
  "prompt_time_and_tokens_info": "Time consumed: {placeholder1} | Tokens:",
  "prompt_data_cannot_recover_after_deletion": "Data cannot be recovered after deletion",
  "prompt_confirm_deletion_input_prompt_key": "If confirmed, please enter the Prompt Key you want to delete:",
  "prompt_historical_image_message_expires_1day": "Historical image messages expire after 1 day; you can view Trace or go to the Prompt development page to debug and retrieve image information",
  "prompt_switch_template_engine": "Switch template engine",
  "prompt_may_cause_variable_render_failure": "May cause existing variable rendering failure, please operate carefully.",
  "prompt_normal_template_engine": "Normal template engine",
  "prompt_triple_braces_variable_recognition": "Triple braces {{{}}} recognize variables",
  "prompt_jinja2_template_engine": "Jinja2 template engine",
  "prompt_manual_add_delete_variables_complex_logic": "Manually add and delete variables, supports complex logic",
  "prompt_user_manual": "User manual",
  "prompt_use_sdk": "Use SDK",
  "prompt_free_comparison_mode": "Free comparison mode",
  "prompt_please_select_a_model": "Please select a model",
  "prompt_response_randomness": "Response randomness",
  "prompt_repetition_penalty": "Repetition penalty",
  "prompt_type_change": "Type change",
  "prompt_model_settings": "Model settings",
  "prompt_template_engine": "Template engine",
  "prompt_multiple_runs": "Multiple runs",
  "prompt_single_run": "Single run",
  "prompt_run_mode": "Run mode",
  "prompt_model_output_single_response": "Model outputs only one response each time",
  "prompt_model_output_multi_response_for_stability_test": "Model outputs multiple responses simultaneously each time, facilitating testing of response stability",
  "prompt_run_group_count": "Number of run groups",
  "prompt_max_upload_MAX_IMAGE_FILE_images": "You can upload up to {MAX_IMAGE_FILE} images",
  "prompt_image_size_max_MAX_FILE_SIZE_MB_MB": "Image size cannot exceed {MAX_FILE_SIZE_MB}MB",
  "prompt_add_function": "Add function",
  "prompt_simulated_value_colon": "Simulated value:",
  "prompt_prompt_debug_data_loading_refresh": "Prompt debug data is on the way, please refresh and try again",
  "prompt_step_placeholder1": "Step {placeholder1}",
  "prompt_add_variable": "Add variable",
  "prompt_please_input_simulated_message": "Please enter simulated message",
  "prompt_please_input_float_max_4_decimal": "Please enter a floating-point number, up to 4 decimal places",
  "prompt_please_input_variable_value": "Please enter variable value",
  "prompt_cannot_add_check_form_data": "Cannot add, please check form data",
  "prompt_variable_name": "Variable name",
  "prompt_please_input_variable_name": "Please enter variable name",
  "prompt_variable_name_cannot_start_space": "Variable name cannot start with a space",
  "prompt_variable_name_format_rule": "Variable name format only supports letters, numbers, underscores, hyphens, and cannot start with a digit",
  "prompt_variable_name_exists": "Variable name already exists",
  "prompt_please_select_variable_data_type": "Please select variable data type",
  "prompt_variable_value": "Variable value",
  "prompt_edit_variable": "Edit variable",
  "prompt_modify_version_tag": "Modify version tag",
  "prompt_create_version_tag_success": "Version tag created successfully",
  "prompt_please_input_version_tag": "Please enter version tag",
  "prompt_tag_allows_lowercase_num_underscore": "Tag only allows lowercase letters, numbers and underscores",
  "prompt_tag_already_exists": "Tag already exists",
  "prompt_tag_length_max_50_chars": "Tag length cannot exceed 50 characters",
  "prompt_tag_exists_in_promptVersion": "The current tag already exists in version {promptVersion}",
  "prompt_create_custom_tag": "Create custom tag",
  "prompt_max_select_MAX_SELECT_COUNT_tags": "You can select up to {MAX_SELECT_COUNT} tags",
  "prompt_version_tag_duplicates_exist": "Duplicate version tags exist",
  "prompt_selected_tags": "Selected tags",
  "prompt_tag_effect_other_versions_submission_success": "Already effective in other versions of the current Prompt. After successful submission, the tag will be effective only in the current version.",
  "prompt_version_tag": "Version tag",
  "prompt_mark_version_feature_sdk_fetch": "Mark version features, you can use the tag to fetch specific Prompt versions via SDK. See details",
  "prompt_kouzi_compass_no_skill_reference_debug": "Kouzi Compass temporarily does not support skill reference and debugging",
  "assistant_role": "Assistant",
  "placeholder_var_error": "The Placeholder variable does not exist or is named incorrectly.",
  "placeholder_var_name": "Placeholder variable name",
  "placeholder_var_execute_error": "The Placeholder variable name does not exist or is named incorrectly",
  "placeholder_var_create_error": "The Placeholder variable name does not exist or is named incorrectly, so creation failed.",
  "prompt_id_inconsistent": "The prompt IDs are inconsistent",
  "prompt_key": "Prompt key",
  "prompt_variable": "Prompt variable",
  "prompt_description": "Prompt description",
  "prompt_name": "Prompt name",
  "prompt_template": "Prompt template",
  "prompt_debug_data_refresh_retry": "Prompt debug data is on the way. Please refresh and try again.",
  "system_role": "System",
  "top_k": "Top K",
  "top_p": "Top P",
  "trace_id": "Trace ID",
  "user_role": "User",
  "version_number_lt_error": "The version number cannot be less than the current version.",
  "incorrect_version_number": "The version number format is incorrect.",
  "submission_no_version_diff": "There is no version difference in this submission.",
  "edit_placeholder": "Edit Placeholder",
  "edit_prompt": "Edit Prompt",
  "variable_setting": "Variable setting",
  "parameter_config": "Parameter config",
  "param_value": "Parameter value",
  "draft_saving": "Draft is being saved...",
  "draft_auto_saved_in": "The draft has been automatically saved in ",
  "insert_variable": "Insert variables",
  "insert_template": "Insert template",
  "create_prompt": "Create Prompt",
  "create_copy": "Copy Prompt",
  "presence_penalty": "Presence penalty",
  "open_enable_function": "Turn on the enabling function.",
  "single_step_debugging": "Step Debug",
  "method_exists": "The current method already exists. Please rename it.",
  "no_draft_change": "Currently no draft changes",
  "x_step": "Step {num}",
  "prompt_effect_evaluation": "Conduct a performance evaluation of the prompt to improve application effectiveness.",
  "revert_draft_version": "Revert to the draft version",
  "method_name_rule": "The method name can only contain letters (a-z, A-Z), digits (0-9), underscores (_), and hyphens (-), with a maximum length of 64 characters.",
  "copy_prompt_key": "Copy Prompt Key",
  "copy_trace_id": "Copy the Trace ID",
  "copy_variable_name": "Copy the variable name",
  "model_not_support_picture": "The model does not support images",
  "close_enable_function": "Turn off the enabling function.",
  "restore_version_tip": "Restoring will overwrite the latest prompt. Confirm restore to this version?",
  "restore_to_this_version": "Restore to this version.",
  "function_call": "Function call",
  "time_consumed": "Time cost",
  "rollback_success": "Rollback succeeded.",
  "load_more": "Load more",
  "confirm_delete_var_in_tpl": "This variable will be removed from the Prompt template. Confirm deletion?",
  "cozeloop_sdk_data_report_observation": "Connect to the CozeLoop SDK to enable data reporting and monitoring.",
  "prompt_key_format": "Only English letters, digits, '_' and '.' are allowed, and the name must start with a letter.",
  "prompt_name_format": "Only English letters, digits, Chinese characters, '-', '_', and '.' are supported, and it must start with English letters, digits, or Chinese characters.",
  "enter_free_comparison_mode": "Comparison mode",
  "quick_create": "Quick creation",
  "historical_data_has_empty_content": "The historical data contains empty content.",
  "go_immediately": "Go now",
  "mock_message": "Mock message",
  "mock_message_group": "Mock message group - {key}",
  "mock_value": "Mock value",
  "model_id": "Model ID",
  "model_not_support": "Not support",
  "model_name": "Model name",
  "model_config": "Model config",
  "model_run_error": "An error occurred during the model operation.",
  "model_runtime_error": "Model operation error",
  "default_mock_value": "Default mock value",
  "frequency_penalty": "Frequency penalty",
  "enable_function": "Enable",
  "clear_history_messages": "Clear historical messages.",
  "request_initiation_time": "Request initiation time",
  "prompt_key_again_confirm": "Please enter the Prompt Key to confirm again.",
  "input_version_number": "Please enter the version number in the format a.b.c, where each segment ranges from 0 to 9999.",
  "please_input_with_vars": "Please enter content and use variables such as '{{USER_NAME}}",
  "input_question_tip": "Please enter a question to test the large language model's response. Press Enter to send, and Shift+Enter for a new line.",
  "add_prompt_tpl_or_input_question": "Please add a Prompt template or enter your question",
  "confirm_delete_current_prompt_template": "Are you sure you want to delete the current Prompt template?",
  "confirm_version_difference": "Confirm the version differences",
  "confirm_version_info": "Confirm the version information",
  "confirm_delete_function": "Are you sure you want to delete this function?",
  "confirm_delete_message": "Are you sure to delete this message?",
  "delete_prompt": "Delete the Prompt",
  "delete_prompt_template": "Delete the Prompt template",
  "delete_variable": "Delete variable",
  "delete_success": "Deletion successful",
  "delete_control_group": "Delete the control group.",
  "delete_function": "Delete function",
  "delete_message": "Delete the message",
  "set_to_reference_group": "Set as the reference group.",
  "deep_thinking": "Deep in thought",
  "collapse_model_and_var_area": "Collapse the model configuration and variable area",
  "collapse_preview_and_debug": "Collapse the preview and debug",
  "input_prompt_key_to_delete": "Enter the Prompt Key you want to delete:",
  "search_prompt_key_or_prompt_name": "Search for Prompt Key or Prompt name",
  "model_not_support_multimodal": "The selected model does not support multimodality. Please adjust the variable type or change the model.",
  "add_message": "Add message",
  "debug_history": "Debug history",
  "stop_respond": "Stop responding",
  "image_size_not_exceed_num_mb": "The size of the picture cannot exceed {num}MB.",
  "image_upload_error": "Image upload failed. Please try again later.",
  "exit_free_comparison_mode": "Exit the free comparison mode",
  "placeholder_name_exists": "The text variable name already exists. Please modify the Placeholder variable name.",
  "new_function": "New function",
  "prompt_version_number_needed": "The Prompt version number needs to be provided.",
  "deeply_thought": "Thought deeply",
  "preview_and_debug": "Preview and Debug",
  "input_mock_value_here": "Enter simulation values here to simulate the return values of the function.",
  "no_prompt": "No Prompt",
  "no_variable": "No variable",
  "no_debug_record": "No debug records now.",
  "add_control_group": "Add control group",
  "expand_model_and_var_area": "Expand the model configuration and variable area",
  "expand_preview_and_debug": "Expand preview and debug",
  "prompt_var_format": "Starts with letter, supports letters and underscores",
  "placeholder_format": "Only English letters and underscores are allowed, and the first character must be a letter",
  "num_words": "Number of characters: {num}",
  "max_tokens": "Max tokens",
  "max_upload_picture_num": "Upload a maximum of {num} pictures.",
  "recent_submission_time": "Recent submission time"
}
